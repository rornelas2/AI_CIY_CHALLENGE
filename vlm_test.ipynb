{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "288bbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"data/annotations/caption/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_caption.json\"\n",
    "video_root = \"data/videos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d20283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rornelas/Desktop/Mi3_Lab/AI_CIY_CHALLENGE/llava-vlm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration,AutoModelForCausalLM \n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac1ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/DAMO-NLP-SG/VideoLLaMA3-2B:\n",
      "- image_processing_videollama3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/DAMO-NLP-SG/VideoLLaMA3-2B:\n",
      "- processing_videollama3.py\n",
      "- image_processing_videollama3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'VideoInput' from 'transformers.image_utils' (/home/rornelas/Desktop/Mi3_Lab/AI_CIY_CHALLENGE/llava-vlm/lib/python3.12/site-packages/transformers/image_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33mDAMO-NLP-SG/VideoLLaMA3-2B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m      5\u001b[39m     model_path,\n\u001b[32m      6\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     attn_implementation=\u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m processor = \u001b[43mAutoProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Mi3_Lab/AI_CIY_CHALLENGE/llava-vlm/lib/python3.12/site-packages/transformers/models/auto/processing_auto.py:371\u001b[39m, in \u001b[36mAutoProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    366\u001b[39m     trust_remote_code = resolve_trust_remote_code(\n\u001b[32m    367\u001b[39m         trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n\u001b[32m    368\u001b[39m     )\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     processor_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocessor_auto_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    375\u001b[39m     processor_class.register_for_auto_class()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Mi3_Lab/AI_CIY_CHALLENGE/llava-vlm/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:581\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    568\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m    569\u001b[39m final_module = get_cached_module_file(\n\u001b[32m    570\u001b[39m     repo_id,\n\u001b[32m    571\u001b[39m     module_file + \u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m     repo_type=repo_type,\n\u001b[32m    580\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Mi3_Lab/AI_CIY_CHALLENGE/llava-vlm/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:277\u001b[39m, in \u001b[36mget_class_in_module\u001b[39m\u001b[34m(class_name, module_path, force_reload)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# reload in both cases, unless the module is already imported and the hash hits\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33m__transformers_module_hash__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) != module_hash:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[43mmodule_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m     module.__transformers_module_hash__ = module_hash\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:995\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/DAMO-NLP-SG/VideoLLaMA3-2B/9f7acdfaa409a2ff841f9a079b6e2731b9e002b2/processing_videollama3.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenization_utils_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTokenizedInput, TextInput\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m image_processing_videollama3\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_processing_videollama3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m         is_valid_image, is_valid_video,\n\u001b[32m     29\u001b[39m     )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/DAMO-NLP-SG/VideoLLaMA3-2B/9f7acdfaa409a2ff841f9a079b6e2731b9e002b2/image_processing_videollama3.py:36\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageInput\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     convert_to_rgb,\n\u001b[32m     33\u001b[39m     resize,\n\u001b[32m     34\u001b[39m     to_channel_dimension_format,\n\u001b[32m     35\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     37\u001b[39m     OPENAI_CLIP_MEAN,\n\u001b[32m     38\u001b[39m     OPENAI_CLIP_STD,\n\u001b[32m     39\u001b[39m     ChannelDimension,\n\u001b[32m     40\u001b[39m     ImageInput,\n\u001b[32m     41\u001b[39m     PILImageResampling,\n\u001b[32m     42\u001b[39m     VideoInput,\n\u001b[32m     43\u001b[39m     get_image_size,\n\u001b[32m     44\u001b[39m     infer_channel_dimension_format,\n\u001b[32m     45\u001b[39m     is_scaled_image,\n\u001b[32m     46\u001b[39m     is_valid_image,\n\u001b[32m     47\u001b[39m     make_list_of_images,\n\u001b[32m     48\u001b[39m     to_numpy_array,\n\u001b[32m     49\u001b[39m )\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorType, is_vision_available, logging\n\u001b[32m     53\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'VideoInput' from 'transformers.image_utils' (/home/rornelas/Desktop/Mi3_Lab/AI_CIY_CHALLENGE/llava-vlm/lib/python3.12/site-packages/transformers/image_utils.py)"
     ]
    }
   ],
   "source": [
    "#Videollama3\n",
    "device = \"cuda:0\"\n",
    "model_path = \"DAMO-NLP-SG/VideoLLaMA3-2B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    device_map={\"\": device},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7abe9773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n",
      "1\n",
      "NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc189d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf351cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 40.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Llava\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a5391c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(frames, prompt):\n",
    "    captions = []\n",
    "    for frame in frames:\n",
    "        inputs = processor(images=frame, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "        caption = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        captions.append(caption)\n",
    "        \n",
    "        return \" \".join(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fc6b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_middle_frame(video_path, start_time, end_time):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"[ERROR] Cannot open video: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 0:\n",
    "        cap.release()\n",
    "        return None\n",
    "    \n",
    "    middle_time = (start_time + end_time) / 2.0\n",
    "    frame_number = int(middle_time * fps)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"[ERROR] Failed to read frame at {frame_number} in video: {video_path}\")\n",
    "        return None\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(frame_rgb)\n",
    "    return pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e466630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_paths, start_time = 0.0, end_time=None, interval=1.0):\n",
    "    all_frames = []\n",
    "    for video_path in video_paths:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Cannot open video: {video_path}\")\n",
    "            continue\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if fps == 0:\n",
    "            cap.release()\n",
    "            continue\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        video_duration = total_frames/fps\n",
    "        \n",
    "        if end_time is None or end_time > video_duration:\n",
    "            end_time = video_duration\n",
    "        \n",
    "        frame_interval = int(fps*interval)\n",
    "        start_frame = int(start_time * fps)\n",
    "        end_frame = int(end_time * fps)\n",
    "\n",
    "        for frame_num in range(start_frame, end_frame, frame_interval):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                continue\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame_rgb)\n",
    "            all_frames.append(pil_image)\n",
    "        cap.release()\n",
    "    return all_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2db44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_video_path(video_root, video_file):\n",
    "    for subfolder in [\"train\", \"val\"]:\n",
    "        folder_path = os.path.join(video_root, subfolder)\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            if video_file in files:\n",
    "                return os.path.join(root, video_file)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2239fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_video(video_root: str, video_file: str):\n",
    "    for dirpath, _, files in os.walk(video_root):\n",
    "        if video_file in files:\n",
    "            return os.path.join(dirpath, video_file)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b6a4c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_json(json_path: str, video_root: str):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    video_files = data.get(\"overhead_videos\", [])\n",
    "    events = data.get(\"event_phase\", [])\n",
    "\n",
    "    if not video_files or not events:\n",
    "        print(f\"[SKIP] Invalid video/event alignment in {json_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Processing: {os.path.basename(json_path)}\")\n",
    "\n",
    "    for event in events:\n",
    "        start = float(event[\"start_time\"])\n",
    "        end = float(event[\"end_time\"])\n",
    "        labels = event.get(\"labels\", [])\n",
    "\n",
    "        all_frames = []\n",
    "        for video_file in video_files:\n",
    "            video_paths = [find_video(video_root, vf) for vf in video_files]\n",
    "            frames = extract_frames(video_paths, start, end)\n",
    "            if frames:\n",
    "                all_frames.extend(frames)\n",
    "\n",
    "        if not all_frames:\n",
    "            continue\n",
    "\n",
    "        caption_ped = generate_caption(\n",
    "            all_frames,\n",
    "            \"<image>  Describe the crash victim in detail: age, gender, clothing, posture, and behavior. Were they distracted or alert? Were they in a legal crossing area? Mention their awareness of the vehicle.\"\n",
    "\n",
    "        )\n",
    "        caption_veh = generate_caption(\n",
    "            all_frames,\n",
    "            \"<image>  Describe the vehicle involved in the accident with the crash victim. Was it accelerating, braking, or turning? Describe its position relative to the pedestrian and crosswalk. Did it yield? Was it following traffic rules?\"\n",
    "        )\n",
    "        print(f\"\\nEvent Labels: {labels}\")\n",
    "        print(f\"Pedestrian: {caption_ped}\")\n",
    "        print(f\"Vehicle: {caption_veh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "11161b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: 20230707_8_SN46_T1_caption.json\n",
      "\n",
      "Event Labels: ['4']\n",
      "Pedestrian: Describe the crash victim in detail: age, gender, clothing, posture, and behavior. Were they distracted or alert? Were they in a legal crossing area? Mention their awareness of the vehicle.\n",
      "\n",
      "The crash victim is a person wearing a white shirt. They are standing in the middle of the street, possibly waiting to cross the road. They are not in a legal crossing area, and their posture suggests that they are alert and aware of their surroundings. It is not clear if they were distracted or not, but their presence in the middle of the street indicates that they were not following traffic rules and could be at risk of being hit by a vehicle.\n",
      "Vehicle: Describe the vehicle involved in the accident with the crash victim. Was it accelerating, braking, or turning? Describe its position relative to the pedestrian and crosswalk. Did it yield? Was it following traffic rules?\n",
      "\n",
      "The vehicle involved in the accident was a black van. It was traveling on the road and was not following traffic rules. The van was not accelerating, braking, or turning. Instead, it was driving straight ahead. The van was positioned on the road, and it was not yielding to the pedestrian who was crossing the street. The pedestrian was crossing the street at a crosswalk, and the van was not following traffic rules, which led to the accident.\n",
      "\n",
      "Event Labels: ['3']\n",
      "Pedestrian: Describe the crash victim in detail: age, gender, clothing, posture, and behavior. Were they distracted or alert? Were they in a legal crossing area? Mention their awareness of the vehicle.\n",
      "\n",
      "The crash victim is a person wearing a white shirt. They are standing in the middle of the street, possibly waiting to cross the road. They are not in a legal crossing area, and their posture suggests that they are alert and aware of their surroundings. It is not clear if they were distracted or not, but their presence in the middle of the street indicates that they were not following traffic rules and could be at risk of being hit by a vehicle.\n",
      "Vehicle: Describe the vehicle involved in the accident with the crash victim. Was it accelerating, braking, or turning? Describe its position relative to the pedestrian and crosswalk. Did it yield? Was it following traffic rules?\n",
      "\n",
      "The vehicle involved in the accident was a black car. It was traveling on the road and was not following traffic rules. The car was not accelerating, braking, or turning. Instead, it was driving straight ahead. The car was positioned in the middle of the road, and it was not yielding to the pedestrian who was crossing the street. The pedestrian was crossing the street at a crosswalk, and the car did not stop for the pedestrian, resulting in a collision.\n",
      "\n",
      "Event Labels: ['2']\n",
      "Pedestrian: Describe the crash victim in detail: age, gender, clothing, posture, and behavior. Were they distracted or alert? Were they in a legal crossing area? Mention their awareness of the vehicle.\n",
      "\n",
      "The crash victim is a woman wearing a pink shirt. She is standing in the middle of the street, possibly waiting to cross the road. She is not in a legal crossing area, and her posture suggests that she is alert and aware of her surroundings. It is not clear if she was distracted or not, but her awareness of the vehicle indicates that she was at least somewhat aware of her environment.\n",
      "Vehicle: Describe the vehicle involved in the accident with the crash victim. Was it accelerating, braking, or turning? Describe its position relative to the pedestrian and crosswalk. Did it yield? Was it following traffic rules?\n",
      "\n",
      "The vehicle involved in the accident was a black van. It was traveling on the road and was not following traffic rules. The van was not accelerating, braking, or turning. Instead, it was driving straight ahead. The van was positioned in the middle of the road, and it was not yielding to the pedestrian who was crossing the street. The pedestrian was crossing the street at a crosswalk, and the van was not following traffic rules, which led to the accident.\n",
      "\n",
      "Event Labels: ['1']\n",
      "Pedestrian: Describe the crash victim in detail: age, gender, clothing, posture, and behavior. Were they distracted or alert? Were they in a legal crossing area? Mention their awareness of the vehicle.\n",
      "\n",
      "The crash victim is a woman wearing a pink shirt. She is standing in the middle of the street, possibly waiting to cross the road. She is not in a legal crossing area, and her posture suggests that she is alert and aware of her surroundings. It is unclear whether she was distracted or not, but her presence in the middle of the street poses a risk to her safety.\n",
      "Vehicle: Describe the vehicle involved in the accident with the crash victim. Was it accelerating, braking, or turning? Describe its position relative to the pedestrian and crosswalk. Did it yield? Was it following traffic rules?\n",
      "\n",
      "The vehicle involved in the accident was a black van. It was traveling on the road and was not following traffic rules. The van was not accelerating, braking, or turning. Instead, it was driving straight ahead. The van was positioned on the road, and it was not yielding to the pedestrian who was crossing the street. The pedestrian was crossing the street at a crosswalk, and the van was not following traffic rules, which led to the accident.\n",
      "\n",
      "Event Labels: ['0']\n",
      "Pedestrian: Describe the crash victim in detail: age, gender, clothing, posture, and behavior. Were they distracted or alert? Were they in a legal crossing area? Mention their awareness of the vehicle.\n",
      "\n",
      "The crash victim is a person wearing a white shirt. They are standing in the middle of the street, possibly distracted or not paying attention to their surroundings. They are not in a legal crossing area and are not aware of the vehicle approaching them.\n",
      "Vehicle: Describe the vehicle involved in the accident with the crash victim. Was it accelerating, braking, or turning? Describe its position relative to the pedestrian and crosswalk. Did it yield? Was it following traffic rules?\n",
      "\n",
      "The vehicle involved in the accident was a black car. It was traveling on the road and was not following traffic rules. The car was not accelerating, braking, or turning. Instead, it was driving straight ahead. The car was positioned on the road, and it was not in the crosswalk. The car was not following traffic rules, and it collided with the pedestrian who was crossing the street.\n"
     ]
    }
   ],
   "source": [
    "#Llava\n",
    "run_on_json(json_path, video_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41a191b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beginning the vqa\n",
    "json_path = \"data/annotations/vqa/train/20230707_8_SN46_T1\"\n",
    "video_root = \"data/videos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "471ea13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vqa_type(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, list) and all('environment' in item for item in data if isinstance(item, dict)):\n",
    "        return \"environment\"\n",
    "    \n",
    "    if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):\n",
    "        item = data[0]\n",
    "        if \"event_phase\" in item:\n",
    "            if \"overhead_videos\" in item:\n",
    "                return \"overhead_view\"\n",
    "            elif \"vehicle_view\" in item:\n",
    "                return \"vehicle_view\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ccf781b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected the question type:  overhead_view\n"
     ]
    }
   ],
   "source": [
    "json_path2 = 'data/annotations/vqa/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1.json'\n",
    "test = find_vqa_type(json_path2)\n",
    "print(\"Detected the question type: \", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de1761ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, image_pil, question, choices, tokenizer, processor):\n",
    "    prompt = \"<image>\\n\" + question + \"\\nChoices:n\"\n",
    "    for key, val in choices.items():\n",
    "        prompt += f\"{key}: {val}\\n\"\n",
    "    prompt += \"Answer with the letter of the correct choice.\"\n",
    "    \n",
    "    inputs = processor(text=prompt, images=image_pil, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "        answer_text = tokenizer.decode(outputs[0], skip_special_tokens=True).lower().strip()\n",
    "    \n",
    "    for letter in ['a', 'b', 'c', 'd']:\n",
    "        if letter in answer_text:\n",
    "            return letter\n",
    "    return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dd6bff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_answer(model, frames, question, choices, processor, tokenizer):\n",
    "    votes = {}\n",
    "    \n",
    "    for frame in frames:\n",
    "        answer = generate_answer(model,frame, question, choices, tokenizer, processor)\n",
    "        if answer:\n",
    "            votes[answer] = votes.get(answer, 0) + 1\n",
    "            \n",
    "    if not votes:\n",
    "        return None\n",
    "\n",
    "    return max(votes.items(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d3f080e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vqa_on_scenario(base_dir, video_root, model):\n",
    "    all_results = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        if any(p in root for p in ['environment', 'overhead_view', 'vehicle_view']):\n",
    "            for file in files:\n",
    "                if file.endswith(\".json\"):\n",
    "                    perspective = os.path.basename(root)\n",
    "                    scenario_id = file.replace(\".json\", \"\")\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    \n",
    "                    print(f\"Processing {perspective}\")\n",
    "                    \n",
    "                    with open(full_path, 'r') as f:\n",
    "                        json_data = json.load(f)\n",
    "                    \n",
    "                    if perspective == \"environment\":\n",
    "                        result = evaluate_environment(json_data, video_root, model)\n",
    "                    elif perspective == \"overhead_view\":\n",
    "                        result = evaluate_overhead(json_data, video_root, model)\n",
    "                    elif perspective == \"vehicle_view\":\n",
    "                        result = evaluate_vehicle(json_data, video_root, model)\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        \"scenario\": scenario_id,\n",
    "                        \"perspective\": perspective,\n",
    "                        \"results\": result\n",
    "                        })   \n",
    "    return all_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b2d39f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vehicle(json_data, video_root, model):\n",
    "    results = []\n",
    "    video_filename = json_data[0].get(\"vehicle_view\")\n",
    "    video_path = find_video(video_root, video_filename)\n",
    "    \n",
    "    for event in json_data[0].get(\"event_phase\", []):\n",
    "        start = float(event[\"start_time\"])\n",
    "        end = float(event[\"end_time\"])\n",
    "        for q in event.get(\"conversations\", []):\n",
    "            question_text = q.get(\"question\")\n",
    "            choices = {k: q[k] for k in ['a', 'b', 'c', 'd'] if k in q}\n",
    "            correct = q.get(\"correct\")\n",
    "            \n",
    "            frames = extract_frames([video_path], start, end)\n",
    "            answer = final_answer(model, frames, question_text, choices, processor, tokenizer)\n",
    "            is_correct = (answer == correct)\n",
    "            \n",
    "            results.append({\n",
    "                \"perspective\": \"vehicle_view\",\n",
    "                \"question\": question_text,\n",
    "                \"choices\": choices,\n",
    "                \"correct\": correct,\n",
    "                \"model_answer\": answer,\n",
    "                \"is_correct\": is_correct,\n",
    "            })\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f4686f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_overhead(json_data, video_root, model):\n",
    "    results = []\n",
    "    overhead_videos = json_data[0].get(\"overhead_videos\", [])\n",
    "    \n",
    "    video_dirname = os.path.basename(json_path).replace(\".json\", \"\")\n",
    "    video_prefix = video_dirname.split(\"_\")[0:4]\n",
    "    video_folder = \"_\".join(video_prefix)\n",
    "    video_dir = os.path.join(video_root, \"train\", video_folder, \"overhead_view\")\n",
    "    videos_path = [os.path.join(video_dir, vid) for vid in overhead_videos]\n",
    "    \n",
    "    event_phases = json_data[0].get(\"event_phase\", [])\n",
    "    for phase in event_phases:\n",
    "        start = float(phase.get(\"start_time\", 0))\n",
    "        end = float(phase.get(\"end_time\", 0))\n",
    "        for conv in phase.get(\"conversations\", []):\n",
    "            question_text = conv.get(\"question\")\n",
    "            correct = conv.get(\"correct\")\n",
    "            choices = {k: conv[k] for k in ['a', 'b', 'c', 'd'] if k in conv}\n",
    "            \n",
    "            frames = extract_frames(videos_path, start, end)\n",
    "            answer = final_answer(model, frames, question_text, choices, processor, tokenizer)\n",
    "            is_correct = (answer == correct)\n",
    "            \n",
    "            results.append({\n",
    "                \"perspective\": \"overhead\",\n",
    "                \"question\": question_text,\n",
    "                \"choices\": choices,\n",
    "                \"correct\": correct,\n",
    "                \"model_answer\": answer,\n",
    "                \"is_correct\": is_correct,\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e7c4df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_environment(json_data, video_root, model):\n",
    "    results = []\n",
    "    overhead_videos = json_data[0].get(\"overhead_videos\", [])\n",
    "    \n",
    "    video_dirname = os.path.basename(json_path).replace(\".json\", \"\")\n",
    "    video_prefix = video_dirname.split(\"_\")[0:4]\n",
    "    video_folder = \"_\".join(video_prefix)\n",
    "    video_dir = os.path.join(video_root, \"train\", video_folder, \"overhead_view\")\n",
    "    videos_path = [os.path.join(video_dir, vid) for vid in overhead_videos]\n",
    "    \n",
    "    questions = json_data[0].get(\"environment\", [])\n",
    "    for q in questions:\n",
    "        question_text = q.get(\"question\")\n",
    "        correct = q.get(\"correct\")\n",
    "        choices = {k: q[k] for k in ['a', 'b', 'c', 'd'] if k in q}\n",
    "    \n",
    "        frames = extract_frames(videos_path)\n",
    "        answer = final_answer(model, frames, question_text, choices, processor, tokenizer)\n",
    "        is_correct = (answer == correct)\n",
    "        \n",
    "        results.append({\n",
    "            \"perspective\": \"environment\",\n",
    "            \"question\": question_text,\n",
    "            \"choices\": choices,\n",
    "            \"correct\": correct,\n",
    "            \"model_answer\": answer,\n",
    "            \"is_correct\": is_correct,\n",
    "        })\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "470adcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing overhead_view\n",
      "Processing vehicle_view\n",
      "Processing environment\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"data/annotations/vqa/train/20230707_8_SN46_T1\"\n",
    "video_root = \"data/videos\"\n",
    "\n",
    "results = run_vqa_on_scenario(base_dir, video_root, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8bd6e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_json(results, output_path):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"[INFO] Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "44cd9d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Results saved to: outputs/full_vqa_results_test.json\n"
     ]
    }
   ],
   "source": [
    "output_json = \"outputs/full_vqa_results_test.json\"\n",
    "save_results_to_json(results, output_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
