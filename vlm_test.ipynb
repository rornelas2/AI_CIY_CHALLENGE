{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "288bbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"data/annotations/caption/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_caption.json\"\n",
    "video_root = \"data/videos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d20283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eac1ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 39.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# LLAVA Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf351cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blip-2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor2 = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "model2 = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5391c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image, prompt):\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "    return processor.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fc6b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_middle_frame(video_path, start_time, end_time):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"[ERROR] Cannot open video: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 0:\n",
    "        cap.release()\n",
    "        return None\n",
    "    \n",
    "    middle_time = (start_time + end_time) / 2.0\n",
    "    frame_number = int(middle_time * fps)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"[ERROR] Failed to read frame at {frame_number} in video: {video_path}\")\n",
    "        return None\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2db44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_video_path(video_root, video_file):\n",
    "    for subfolder in [\"train\", \"val\"]:\n",
    "        folder_path = os.path.join(video_root, subfolder)\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            if video_file in files:\n",
    "                return os.path.join(root, video_file)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2239fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_video(video_root: str, video_file: str):\n",
    "    for dirpath, _, files in os.walk(video_root):\n",
    "        if video_file in files:\n",
    "            return os.path.join(dirpath, video_file)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e048fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_json(json_path: str, video_root: str):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    video_file = data.get(\"overhead_videos\", [None])[0]\n",
    "    if not video_file:\n",
    "        print(\"[SKIP] No video listed in JSON.\")\n",
    "        return\n",
    "\n",
    "    video_path = find_video(video_root, video_file)\n",
    "    if not video_path:\n",
    "        print(f\"[ERROR] Could not find video file: {video_file}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Processing: {os.path.basename(json_path)}\")\n",
    "    for event in data.get(\"event_phase\", []):\n",
    "        start = float(event[\"start_time\"])\n",
    "        end = float(event[\"end_time\"])\n",
    "        labels = event.get(\"labels\", [])\n",
    "\n",
    "        frame = extract_middle_frame(video_path, start, end)\n",
    "        if frame is None:\n",
    "            continue\n",
    "\n",
    "        caption_ped = generate_caption(frame, \"<image> Describe the pedestrian behhavior at the time of the crash.\")\n",
    "        caption_veh = generate_caption(frame, \"<image> Describe the vehicle behavior at the time of the crash.\")\n",
    "\n",
    "        print(f\"\\nLabels for video {video_path}\")\n",
    "        print(f\"Labels: {labels}\")\n",
    "        print(f\"üßç Pedestrian: {caption_ped}\")\n",
    "        print(f\"üöó Vehicle: {caption_veh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede65fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: 20230707_8_SN46_T1_caption.json\n",
      "\n",
      "Labels for video data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4\n",
      "Labels: ['4']\n",
      "üßç Pedestrian: a pedestrian is a person who is walking or riding a bicycle\n",
      "üöó Vehicle: Describe the vehicle's speed, direction, and speed at the time of the crash\n",
      "\n",
      "Labels for video data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4\n",
      "Labels: ['3']\n",
      "üßç Pedestrian: a pedestrian is a person who is walking or riding a bicycle\n",
      "üöó Vehicle: Describe the vehicle's speed, direction, and speed at the time of the crash\n",
      "\n",
      "Labels for video data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4\n",
      "Labels: ['2']\n",
      "üßç Pedestrian: Describe the pedestrian's behavior at the time of the crash\n",
      "üöó Vehicle: Describe the vehicle's speed, direction, and speed at the time of the crash\n",
      "\n",
      "Labels for video data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4\n",
      "Labels: ['1']\n",
      "üßç Pedestrian: a pedestrian is a person who is walking or riding a bicycle\n",
      "üöó Vehicle: Describe the vehicle's speed, direction, and speed at the time of the crash\n",
      "\n",
      "Labels for video data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4\n",
      "Labels: ['0']\n",
      "üßç Pedestrian: Describe the pedestrian's behavior at the time of the crash\n",
      "üöó Vehicle: Describe the vehicle's speed, direction, and speed at the time of the crash\n"
     ]
    }
   ],
   "source": [
    "#Blip-2\n",
    "run_on_json(json_path, video_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11161b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: 20230707_8_SN46_T1_caption.json\n",
      "\n",
      "Labels for video data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4\n",
      "Labels: ['4']\n",
      "üßç Pedestrian:  Describe the pedestrian behhavior at the time of the crash.\n",
      "\n",
      "In the image, there are several cars and a truck on the road, and a dog is also present. The dog is located near the center of the scene, and it appears to be walking or standing on the road. The cars and truck are positioned around the dog, with some cars closer to the dog and others further away. The scene suggests that the dog might be wandering onto the road, which could potentially lead to an accident if drivers are not cautious\n",
      "üöó Vehicle:  Describe the vehicle behavior at the time of the crash.\n",
      "\n",
      "The car is stopped at the intersection, waiting for the traffic light to change.\n",
      "\n",
      "Labels for video data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4\n",
      "Labels: ['3']\n",
      "üßç Pedestrian:  Describe the pedestrian behhavior at the time of the crash.\n",
      "\n",
      "In the image, there is a car crash at an intersection, with a dog standing in the middle of the street. The dog appears to be unharmed, and it is not clear if the dog was involved in the crash or if it just happened to be in the middle of the street. The scene also includes a traffic light, a truck, and a few other cars. The dog is standing in the middle of the street, and it is not clear if it is a\n",
      "üöó Vehicle:  Describe the vehicle behavior at the time of the crash.\n",
      "\n",
      "The vehicle is stopped at the intersection, waiting for the traffic light to change.\n",
      "\n",
      "Labels for video data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4\n",
      "Labels: ['2']\n",
      "üßç Pedestrian:  Describe the pedestrian behhavior at the time of the crash.\n",
      "\n",
      "The pedestrian is walking across the street, and there is a car approaching the intersection. The pedestrian is not paying attention to the traffic, and the car is not stopping for the pedestrian. This lack of attention and adherence to traffic rules can lead to accidents and injuries. It is essential for pedestrians to be aware of their surroundings and follow traffic signals to ensure their safety and the safety of others.\n",
      "üöó Vehicle:  Describe the vehicle behavior at the time of the crash.\n",
      "\n",
      "The vehicle is stopped at a red light.\n",
      "\n",
      "Labels for video data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4\n",
      "Labels: ['1']\n",
      "üßç Pedestrian:  Describe the pedestrian behhavior at the time of the crash.\n",
      "\n",
      "In the image, there is a car crash at an intersection, with a dog standing in the middle of the street. The dog appears to be unharmed, and there are no people around the scene. The intersection is equipped with traffic lights, and the car is positioned on the left side of the scene. There are also a few other cars in the background, but they are not involved in the crash. The scene seems to be captured in a time-lapse photo,\n",
      "üöó Vehicle:  Describe the vehicle behavior at the time of the crash.\n",
      "\n",
      "The image shows a street scene with a car crashing into a traffic light. The car is positioned on the left side of the scene, and the traffic light is located on the right side. There are several people in the scene, with one person standing close to the car, and others scattered around the area. A dog can also be seen in the scene, possibly wandering around the area. The street appears to be a busy intersection, with multiple cars and traffic lights present.\n",
      "\n",
      "Labels for video data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4\n",
      "Labels: ['0']\n",
      "üßç Pedestrian:  Describe the pedestrian behhavior at the time of the crash.\n",
      "\n",
      "In the image, there is a car crash at an intersection, with a dog standing in the middle of the street. The dog appears to be unharmed, and it is not clear if the dog was involved in the crash or if it just happened to be in the middle of the street. There are several people around the scene, some of whom are standing near the car crash, while others are walking or standing further away. The presence of multiple people in the area suggests that the\n",
      "üöó Vehicle:  Describe the vehicle behavior at the time of the crash.\n",
      "\n",
      "The image shows a car crashing into a traffic light at an intersection. The car is positioned on the left side of the intersection, and the traffic light is located on the right side. There are several other cars in the scene, with one car on the right side of the intersection and two cars on the left side. Additionally, there are two people in the scene, one near the center of the intersection and another on the right side. The scene appears to be captured in a\n"
     ]
    }
   ],
   "source": [
    "#Llava\n",
    "run_on_json(json_path, video_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "~vlm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
