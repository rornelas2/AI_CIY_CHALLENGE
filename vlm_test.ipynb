{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "288bbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"data/annotations/caption/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_caption.json\"\n",
    "video_root = \"data/videos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83d20283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration,AutoModelForCausalLM , AutoModelForImageTextToText, Qwen2_5_VLForConditionalGeneration\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eac1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Videollama3\n",
    "device = \"cuda:0\"\n",
    "model_path = \"DAMO-NLP-SG/VideoLLaMA3-2B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    device_map={\"\": device},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7abe9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.is_available())\n",
    "# print(torch.version.cuda)\n",
    "# print(torch.cuda.device_count())\n",
    "# if torch.cuda.is_available():\n",
    "#     print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc189d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf351cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 37.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Llava\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer = AutoProcessor.from_pretrained(model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4b37df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:26<00:00, 43.42s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "#Space Om\n",
    "model_id = \"remyxai/SpaceOm\"\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e51a8808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "#Space lava\n",
    "\n",
    "model_id = \"salma-remyx/spacellava-1.5-7b\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"salma-remyx/spacellava-1.5-7b\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"salma-remyx/spacellava-1.5-7b\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer = AutoProcessor.from_pretrained(model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5391c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(frames, prompt):\n",
    "    captions = []\n",
    "    for frame in frames:\n",
    "        inputs = processor(images=frame, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "        inputs = {k: v.to(torch.bfloat16) if v.dtype == torch.float32 else v for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "        caption = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        captions.append(caption)\n",
    "        \n",
    "    return \" \".join(captions)\n",
    "\n",
    "# def generate_caption_batch(frames, prompt):\n",
    "#     # Process all frames together with the prompt\n",
    "#     inputs = processor(images=frames, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "#     # Convert float tensors to bfloat16 if needed to match model dtype\n",
    "#     inputs = {\n",
    "#         k: (v.to(torch.bfloat16) if isinstance(v, torch.Tensor) and v.dtype == torch.float32 else v.to(model.device) if isinstance(v, torch.Tensor) else v)\n",
    "#         for k, v in inputs.items()\n",
    "#     }\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "        \n",
    "#     caption = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "#     return caption\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc6b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_middle_frame(video_path, start_time, end_time):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"[ERROR] Cannot open video: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 0:\n",
    "        cap.release()\n",
    "        return None\n",
    "    \n",
    "    middle_time = (start_time + end_time) / 2.0\n",
    "    frame_number = int(middle_time * fps)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"[ERROR] Failed to read frame at {frame_number} in video: {video_path}\")\n",
    "        return None\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(frame_rgb)\n",
    "    return pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e466630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_paths, start_time = 0.0, end_time=None, interval=1.0):\n",
    "    all_frames = []\n",
    "    for video_path in video_paths:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Cannot open video: {video_path}\")\n",
    "            continue\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if fps == 0:\n",
    "            cap.release()\n",
    "            continue\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        video_duration = total_frames/fps\n",
    "        \n",
    "        if end_time is None or end_time > video_duration:\n",
    "            end_time = video_duration\n",
    "        \n",
    "        frame_interval = int(fps*interval)\n",
    "        start_frame = int(start_time * fps)\n",
    "        end_frame = int(end_time * fps)\n",
    "\n",
    "        for frame_num in range(start_frame, end_frame, frame_interval):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                continue\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame_rgb)\n",
    "            all_frames.append(pil_image)\n",
    "        cap.release()\n",
    "    return all_frames\n",
    "\n",
    "# def extract_frames(video_paths, start_time=0.0, end_time=None, interval=1.0):\n",
    "#     all_frames = []\n",
    "#     for video_path in video_paths:\n",
    "#         cap = cv2.VideoCapture(str(video_path))\n",
    "#         if not cap.isOpened():\n",
    "#             print(f\"Cannot open video: {video_path}\")\n",
    "#             continue\n",
    "        \n",
    "#         fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "#         if fps == 0:\n",
    "#             cap.release()\n",
    "#             continue\n",
    "        \n",
    "#         total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#         video_duration = total_frames / fps\n",
    "        \n",
    "#         if end_time is None or end_time > video_duration:\n",
    "#             end_time = video_duration\n",
    "        \n",
    "#         frame_interval = int(fps * interval)\n",
    "#         start_frame = int(start_time * fps)\n",
    "#         end_frame = int(end_time * fps)\n",
    "\n",
    "#         for frame_num in range(start_frame, end_frame, frame_interval):\n",
    "#             cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "#             success, frame = cap.read()\n",
    "#             if not success:\n",
    "#                 continue\n",
    "#             frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#             pil_image = Image.fromarray(frame_rgb)\n",
    "\n",
    "#             # Append a dict with metadata + image\n",
    "#             all_frames.append({\n",
    "#                 \"video_path\": video_path,\n",
    "#                 \"fps\": fps,\n",
    "#                 \"frame_num\": frame_num,\n",
    "#                 \"image\": pil_image\n",
    "#             })\n",
    "#         cap.release()\n",
    "#     return all_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2db44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_video_path(video_root, video_file):\n",
    "    for subfolder in [\"train\", \"val\"]:\n",
    "        folder_path = os.path.join(video_root, subfolder)\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            if video_file in files:\n",
    "                return os.path.join(root, video_file)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2239fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_video(video_root: str, video_file: str):\n",
    "    for dirpath, _, files in os.walk(video_root):\n",
    "        if video_file in files:\n",
    "            return os.path.join(dirpath, video_file)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a4c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_on_json(json_path: str, video_root: str):\n",
    "#     with open(json_path, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     video_files = data.get(\"overhead_videos\", [])\n",
    "#     events = data.get(\"event_phase\", [])\n",
    "\n",
    "#     if not video_files or not events:\n",
    "#         print(f\"[SKIP] Invalid video/event alignment in {json_path}\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"[INFO] Processing: {os.path.basename(json_path)}\")\n",
    "\n",
    "#     for event in events:\n",
    "#         start = float(event[\"start_time\"])\n",
    "#         end = float(event[\"end_time\"])\n",
    "#         labels = event.get(\"labels\", [])\n",
    "\n",
    "#         all_frames = []\n",
    "#         for video_file in video_files:\n",
    "#             video_paths = [find_video(video_root, vf) for vf in video_files]\n",
    "#             frames = extract_frames(video_paths, start, end)\n",
    "#             if frames:\n",
    "#                 all_frames.extend(frames)\n",
    "\n",
    "#         if not all_frames:\n",
    "#             continue\n",
    "\n",
    "#         caption_ped = generate_caption(\n",
    "#             all_frames,\n",
    "#             \"<image>  Describe the crash victim in detail: age, gender, clothing, posture, and behavior. Were they distracted or alert? Were they in a legal crossing area? Mention their awareness of the vehicle.\"\n",
    "\n",
    "#         )\n",
    "#         caption_veh = generate_caption(\n",
    "#             all_frames,\n",
    "#             \"<image>  Describe the vehicle involved in the accident with the crash victim. Was it accelerating, braking, or turning? Describe its position relative to the pedestrian and crosswalk. Did it yield? Was it following traffic rules?\"\n",
    "#         )\n",
    "#         print(f\"\\nEvent Labels: {labels}\")\n",
    "#         print(f\"Pedestrian: {caption_ped}\")\n",
    "#         print(f\"Vehicle: {caption_veh}\")\n",
    "\n",
    "def run_on_json(json_path: str, video_root: str):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    video_files = data.get(\"overhead_videos\", [])\n",
    "    events = data.get(\"event_phase\", [])\n",
    "\n",
    "    if not video_files or not events:\n",
    "        print(f\"[SKIP] Invalid video/event alignment in {json_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Processing: {os.path.basename(json_path)}\")\n",
    "\n",
    "    video_paths = [find_video(video_root, vf) for vf in video_files]\n",
    "\n",
    "\n",
    "    for event in events:\n",
    "        start = float(event[\"start_time\"])\n",
    "        end = float(event[\"end_time\"])\n",
    "        labels = event.get(\"labels\", [])\n",
    "\n",
    "        all_frames = []\n",
    "        for video_path in video_paths:\n",
    "            frames = extract_frames([video_path], start, end)\n",
    "            if frames:\n",
    "                all_frames.extend(frames)\n",
    "\n",
    "        if not all_frames:\n",
    "            continue\n",
    "        \n",
    "        pedestrian_prompt = ( \"<image>\"\n",
    "        \" Describe the crash victim: age, gender, clothing, posture, \"\n",
    "        \"behavior, alertness, and crossing legality.\"\n",
    "        )\n",
    "        vehicle_prompt = ( \"<image>\"\n",
    "        \" Describe the vehicle involved: movement, position relative to pedestrian, \"\n",
    "        \"and compliance with traffic rules.\"\n",
    "        )\n",
    "\n",
    "        caption_ped = generate_caption(all_frames, pedestrian_prompt)\n",
    "        caption_veh = generate_caption(all_frames, vehicle_prompt)\n",
    "\n",
    "        print(f\"\\nEvent Labels: {labels}\")\n",
    "        print(f\"Pedestrian: {caption_ped}\")\n",
    "        print(f\"Vehicle: {caption_veh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2434afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions(json_path, video_root):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    video_files = data.get(\"overhead_videos\", [])\n",
    "    events = data.get(\"event_phase\", [])\n",
    "\n",
    "    if not video_files or not events:\n",
    "        print(f\"[SKIP] Invalid video/event alignment in {json_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Processing: {os.path.basename(json_path)}\")\n",
    "\n",
    "    video_paths = [find_video(video_root, vf) for vf in video_files]\n",
    "\n",
    "    for event in events:\n",
    "        start = float(event[\"start_time\"])\n",
    "        end = float(event[\"end_time\"])\n",
    "        labels = event.get(\"labels\", [])\n",
    "\n",
    "        all_frames = []\n",
    "        for video_path in video_paths:\n",
    "            frames = extract_frames([video_path], start, end)\n",
    "            if frames:\n",
    "                all_frames.extend(frames)\n",
    "\n",
    "        if not all_frames:\n",
    "            continue\n",
    "\n",
    "        all_responses = []\n",
    "\n",
    "        for i, segment in enumerate(all_frames):\n",
    "            video_path = segment[\"video_path\"]\n",
    "            fps = segment.get(\"fps\", 30)  # default fps\n",
    "            max_frames = segment.get(\"max_frames\", int(fps * (end - start)))  # better default max_frames\n",
    "\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "                    You are an AI assistant analyzing traffic video segments. \n",
    "                    For each video segment, generate two captions:\n",
    "                    1. Describe pedestrians' behavior, positions, and interactions.\n",
    "                    2. Describe vehicles' movements, positions, and interactions.\n",
    "                    Provide clear, concise captions focusing on relevant traffic events, including accidents or normal traffic flow.\n",
    "                    Use relative positions and timing if applicable.\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"video\", \"video\": {\"video_path\": video_path, \"fps\": fps, \"max_frames\": max_frames}},\n",
    "                        {\"type\": \"text\", \"text\": \"\"\"\n",
    "                        For this video segment, provide two captions:\n",
    "                        Pedestrian Caption: [Describe all pedestrian activity]\n",
    "                        Vehicle Caption: [Describe all vehicle activity]\n",
    "                        Include mentions of any accidents or noteworthy behavior.\n",
    "                        \"\"\"}\n",
    "                    ]\n",
    "                },\n",
    "            ]\n",
    "\n",
    "            inputs = processor(\n",
    "                conversation=conversation,\n",
    "                add_system_prompt=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "            if \"pixel_values\" in inputs:\n",
    "                inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(torch.bfloat16)\n",
    "\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "            response = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "            pedestrian_caption = \"\"\n",
    "            vehicle_caption = \"\"\n",
    "\n",
    "            # Parse the response\n",
    "            for line in response.splitlines():\n",
    "                if line.lower().startswith(\"pedestrian caption:\"):\n",
    "                    pedestrian_caption = line[len(\"pedestrian caption:\"):].strip()\n",
    "                elif line.lower().startswith(\"vehicle caption:\"):\n",
    "                    vehicle_caption = line[len(\"vehicle caption:\"):].strip()\n",
    "\n",
    "            if not pedestrian_caption or not vehicle_caption:\n",
    "                parts = response.split('\\n\\n')\n",
    "                if len(parts) >= 2:\n",
    "                    pedestrian_caption = parts[0].strip()\n",
    "                    vehicle_caption = parts[1].strip()\n",
    "\n",
    "            all_responses.append({\n",
    "                \"segment_index\": i,\n",
    "                \"video_path\": video_path,\n",
    "                \"pedestrian_caption\": pedestrian_caption,\n",
    "                \"vehicle_caption\": vehicle_caption,\n",
    "                \"raw_response\": response,\n",
    "            })\n",
    "\n",
    "        # You can choose to return all_responses here or accumulate them elsewhere\n",
    "        return all_responses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0408c6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: 20230707_8_SN46_T1_caption.json\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Image' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate_captions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_root\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mgenerate_captions\u001b[39m\u001b[34m(json_path, video_root)\u001b[39m\n\u001b[32m     30\u001b[39m all_responses = []\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, segment \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_frames):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     video_path = \u001b[43msegment\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvideo_path\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     34\u001b[39m     fps = segment.get(\u001b[33m\"\u001b[39m\u001b[33mfps\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m30\u001b[39m)  \u001b[38;5;66;03m# default fps\u001b[39;00m\n\u001b[32m     35\u001b[39m     max_frames = segment.get(\u001b[33m\"\u001b[39m\u001b[33mmax_frames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mint\u001b[39m(fps * (end - start)))  \u001b[38;5;66;03m# better default max_frames\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'Image' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "generate_captions(json_path, video_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11161b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Space LLAVA\n",
    "from PIL import Image\n",
    "def load_video_frames(folder_path, num_frames=5):\n",
    "    frames = sorted(os.listdir(folder_path))[:num_frames]\n",
    "    images = [Image.open(os.path.join(folder_path, f)).convert(\"RGB\") for f in frames]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "810cb7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " \n",
      " Please describe the interested pedestrian in the video. Provide information about age, height, clothing, and awareness of surroundings. ASSISTANT: The interested pedestrian in the video is a woman who is crossing the street at a crosswalk. She is wearing a white shirt and black pants, and she appears to be paying attention to her surroundings. The woman is crossing the street at a designated crosswalk, ensuring her safety while navigating the urban environment. \n",
      "\n",
      "The presence of a traffic light in the image indicates that the area is designed to manage traffic flow and pedestrian safety. The woman's awareness of her surroundings and her use of the crosswalk demonstrate responsible behavior and adherence to traffic rules. \n",
      "\n",
      "In the image, there are also two bicycles, which might suggest that the area is bike-friendly or that the woman is using a bicycle as her mode of transportation. Overall, the scene depicts a typical urban environment with pedestrians, traffic, and designated crosswalks for safe navigation. \n",
      "\n",
      "The woman's awareness of her surroundings and her use of the crosswalk highlight the importance of pedestrian safety and responsible behavior in urban settings. \n",
      "\n",
      "ë²ˆì—­\n"
     ]
    }
   ],
   "source": [
    "from decord import VideoReader\n",
    "from decord import cpu\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "video_path = 'data/videos/train/20230707_12_SN17_T1/overhead_view/20230707_12_SN17_T1_Camera1_0.mp4'\n",
    "num_frames = 5\n",
    "frames = [Image.fromarray(vr[i].asnumpy()).convert(\"RGB\") for i in frame_indices]\n",
    "\n",
    "# Adjust prompt to have 5 <image> tokens\n",
    "prompt = \"\\n\".join([\"<image>\"] * num_frames) + \"Please describe the interested pedestrian in the video. Provide information about age, height, clothing, and awareness\"\n",
    "\n",
    "# Load video and extract evenly spaced frames\n",
    "vr = VideoReader(video_path, ctx=cpu(0))\n",
    "total_frames = len(vr)\n",
    "frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]\n",
    "frames = [Image.fromarray(vr[i].asnumpy()).convert(\"RGB\") for i in frame_indices]\n",
    "\n",
    "# Process inputs (make sure processor and model are loaded)\n",
    "inputs = processor(text=prompt, images=frames, return_tensors=\"pt\").to(model.device)\n",
    "output_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "output = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4077c6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']\n"
     ]
    }
   ],
   "source": [
    "print(processor.tokenizer.additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ceace543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " system\n",
      "You are VL-Thinking ðŸ¤”, a helpful assistant with excellent reasoning ability. You should first think about the reasoning process and then provide the answer. Use <think>...</think> and <answer>...</answer> tags.\n",
      "user\n",
      "Please describe the interested pedestrian in the video. Provide specific numerical information about age, height, clothing(color, type), and awareness, position with respect to the vehicle at the time of the accident\n",
      "assistant\n",
      "<think>\n",
      "The interested pedestrian is a young adult male wearing a black jacket and dark pants. He appears to be crossing the street at a pedestrian crosswalk. The pedestrian is positioned near the center of the image, slightly to the left of the crosswalk. His awareness seems to be high as he is looking around, possibly checking for oncoming traffic or pedestrians. The car in the foreground is a silver sedan, and it appears to be approaching from the left side of the frame.\n",
      "</think>\n",
      "<answer>\n",
      "The interested pedestrian is a young adult male, approximately 25-30 years old, wearing a black jacket and dark pants. He is standing near the center of the image, slightly to the left of the crosswalk, and appears to be looking around, possibly checking for oncoming traffic or pedestrians. The car in the foreground is a silver sedan, and it is approaching from the left side of the frame.\n",
      "</answer>\n"
     ]
    }
   ],
   "source": [
    "#Space Om\n",
    "video_path = \"data/videos/train/20230707_12_SN17_T1/overhead_view/20230707_12_SN17_T1_Camera1_0.mp4\"\n",
    "num_frames = 5\n",
    "vr = VideoReader(video_path, ctx=cpu(0))\n",
    "total_frames = len(vr)\n",
    "frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]\n",
    "frames = [Image.fromarray(vr[i].asnumpy()).convert(\"RGB\") for i in frame_indices]\n",
    "\n",
    "# Resize frames (optional, if large)\n",
    "for i in range(len(frames)):\n",
    "    if frames[i].width > 512:\n",
    "        ratio = frames[i].height / frames[i].width\n",
    "        frames[i] = frames[i].resize((512, int(512 * ratio)), Image.Resampling.LANCZOS)\n",
    "\n",
    "# Define system + user chat format\n",
    "system_message = (\n",
    "    \"You are VL-Thinking ðŸ¤”, a helpful assistant with excellent reasoning ability. \"\n",
    "    \"You should first think about the reasoning process and then provide the answer. \"\n",
    "    \"Use <think>...</think> and <answer>...</answer> tags.\"\n",
    ")\n",
    "prompt = \"Please describe the interested pedestrian in the video. Provide specific numerical information about age, height, clothing(color, type), and awareness, position with respect to the vehicle at the time of the accident\"\n",
    "\n",
    "# Format as multi-modal chat\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": img} for img in frames] +\n",
    "                                [{\"type\": \"text\", \"text\": prompt}]}\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text_input = processor.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Process input\n",
    "inputs = processor(text=[text_input], images=frames, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "output_ids = model.generate(**inputs, max_new_tokens=700)\n",
    "output = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Response:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35f04637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: 20230707_8_SN46_T1_caption.json\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Number of images does not match the number of image tokens in the text.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#VideoLlama3-2b\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mrun_on_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_root\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mrun_on_json\u001b[39m\u001b[34m(json_path, video_root)\u001b[39m\n\u001b[32m     72\u001b[39m pedestrian_prompt = ( \u001b[33m\"\u001b[39m\u001b[33m<image>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[33m\"\u001b[39m\u001b[33m Describe the crash victim: age, gender, clothing, posture, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mbehavior, alertness, and crossing legality.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m )\n\u001b[32m     76\u001b[39m vehicle_prompt = ( \u001b[33m\"\u001b[39m\u001b[33m<image>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[33m\"\u001b[39m\u001b[33m Describe the vehicle involved: movement, position relative to pedestrian, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mand compliance with traffic rules.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     79\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m caption_ped = \u001b[43mgenerate_caption_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpedestrian_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m caption_veh = generate_caption_batch(all_frames, vehicle_prompt)\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvent Labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mgenerate_caption_batch\u001b[39m\u001b[34m(frames, prompt)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_caption_batch\u001b[39m(frames, prompt):\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Process all frames together with the prompt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     inputs = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.to(model.device)\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Convert float tensors to bfloat16 if needed to match model dtype\u001b[39;00m\n\u001b[32m     18\u001b[39m     inputs = {\n\u001b[32m     19\u001b[39m         k: (v.to(torch.bfloat16) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor) \u001b[38;5;129;01mand\u001b[39;00m v.dtype == torch.float32 \u001b[38;5;28;01melse\u001b[39;00m v.to(model.device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m v)\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()\n\u001b[32m     21\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/DAMO-NLP-SG/VideoLLaMA3-2B/9f7acdfaa409a2ff841f9a079b6e2731b9e002b2/processing_videollama3.py:708\u001b[39m, in \u001b[36mVideollama3Qwen2Processor.__call__\u001b[39m\u001b[34m(self, text, conversation, images, return_labels, **kwargs)\u001b[39m\n\u001b[32m    706\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou cannot provide \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m\u001b[33m with \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_conversation(conversation, images, return_labels, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_plain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/DAMO-NLP-SG/VideoLLaMA3-2B/9f7acdfaa409a2ff841f9a079b6e2731b9e002b2/processing_videollama3.py:657\u001b[39m, in \u001b[36mVideollama3Qwen2Processor._process_plain\u001b[39m\u001b[34m(self, text, images, return_labels, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    655\u001b[39m     image_inputs = {}\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m text_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BatchFeature(data={**text_inputs, **image_inputs})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/DAMO-NLP-SG/VideoLLaMA3-2B/9f7acdfaa409a2ff841f9a079b6e2731b9e002b2/processing_videollama3.py:691\u001b[39m, in \u001b[36mVideollama3Qwen2Processor.process_text\u001b[39m\u001b[34m(self, text, image_inputs, **kwargs)\u001b[39m\n\u001b[32m    688\u001b[39m         image_idx += \u001b[32m1\u001b[39m\n\u001b[32m    689\u001b[39m     text = text.replace(\u001b[33m\"\u001b[39m\u001b[33m<placeholder>\u001b[39m\u001b[33m\"\u001b[39m, DEFAULT_IMAGE_TOKEN)\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(grid_sizes) == image_idx, \u001b[33m\"\u001b[39m\u001b[33mNumber of images does not match the number of image tokens in the text.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    693\u001b[39m text_inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(text, **kwargs)\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text_inputs\n",
      "\u001b[31mAssertionError\u001b[39m: Number of images does not match the number of image tokens in the text."
     ]
    }
   ],
   "source": [
    "#VideoLlama3-2b\n",
    "run_on_json(json_path, video_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41a191b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beginning the vqa\n",
    "json_path = \"data/annotations/vqa/train/20230707_8_SN46_T1\"\n",
    "video_root = \"data/videos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "471ea13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vqa_type(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, list) and all('environment' in item for item in data if isinstance(item, dict)):\n",
    "        return \"environment\"\n",
    "    \n",
    "    if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):\n",
    "        item = data[0]\n",
    "        if \"event_phase\" in item:\n",
    "            if \"overhead_videos\" in item:\n",
    "                return \"overhead_view\"\n",
    "            elif \"vehicle_view\" in item:\n",
    "                return \"vehicle_view\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccf781b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected the question type:  overhead_view\n"
     ]
    }
   ],
   "source": [
    "json_path2 = 'data/annotations/vqa/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1.json'\n",
    "test = find_vqa_type(json_path2)\n",
    "print(\"Detected the question type: \", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1761ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_spaceom(model, image_pil, question, choices, tokenizer, processor):\n",
    "    # Compose system message (optional, but recommended for SpaceOm)\n",
    "    system_message = (\n",
    "        \"You are VL-Thinking ðŸ¤”, a helpful assistant with excellent reasoning ability.\"\n",
    "        \" Answer by choosing the correct letter from the options.\"\n",
    "    )\n",
    "\n",
    "    # Build chat with multiple image inputs\n",
    "    user_content = []\n",
    "\n",
    "    for frame in frames:\n",
    "        user_content.append({\"type\": \"image\", \"image\": frame})\n",
    "\n",
    "    # Add question and choices as text blocks\n",
    "    choices_text = \"\\n\".join(f\"{key}: {val}\" for key, val in choices.items())\n",
    "    user_content.extend([\n",
    "        {\"type\": \"text\", \"text\": question},\n",
    "        {\"type\": \"text\", \"text\": \"Choices:\\n\" + choices_text},\n",
    "        {\"type\": \"text\", \"text\": \"Answer with the letter of the correct choice.\"}\n",
    "    ])\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    text_input = processor.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Tokenize with multiple images\n",
    "    inputs = processor(text=[text_input], images=frames, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate and decode\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "    answer_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].lower().strip()\n",
    "\n",
    "    # Match any valid choice letter in the output\n",
    "    for letter in choices.keys():\n",
    "        if letter.lower() in answer_text:\n",
    "            return letter.lower()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd6bff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_answer(model, frames, question, choices, processor, tokenizer):\n",
    "    votes = {}\n",
    "    \n",
    "    for frame in frames:\n",
    "        answer = generate_answer_spaceom(model,frame, question, choices, tokenizer, processor)\n",
    "        if answer:\n",
    "            votes[answer] = votes.get(answer, 0) + 1\n",
    "            \n",
    "    if not votes:\n",
    "        return None\n",
    "\n",
    "    return max(votes.items(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3f080e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vqa_on_scenario(base_dir, video_root, model):\n",
    "    all_results = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        if any(p in root for p in ['environment', 'overhead_view', 'vehicle_view']):\n",
    "            for file in files:\n",
    "                if file.endswith(\".json\"):\n",
    "                    perspective = os.path.basename(root)\n",
    "                    scenario_id = file.replace(\".json\", \"\")\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    \n",
    "                    print(f\"Processing {perspective}\")\n",
    "                    \n",
    "                    with open(full_path, 'r') as f:\n",
    "                        json_data = json.load(f)\n",
    "                    \n",
    "                    if perspective == \"environment\":\n",
    "                        result = evaluate_environment(json_data, video_root, model)\n",
    "                    elif perspective == \"overhead_view\":\n",
    "                        result = evaluate_overhead(json_data, video_root, model)\n",
    "                    elif perspective == \"vehicle_view\":\n",
    "                        result = evaluate_vehicle(json_data, video_root, model)\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        \"scenario\": scenario_id,\n",
    "                        \"perspective\": perspective,\n",
    "                        \"results\": result\n",
    "                        })   \n",
    "    return all_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2d39f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vehicle(json_data, video_root, model):\n",
    "    results = []\n",
    "    video_filename = json_data[0].get(\"vehicle_view\")\n",
    "    video_path = find_video(video_root, video_filename)\n",
    "    \n",
    "    for event in json_data[0].get(\"event_phase\", []):\n",
    "        start = float(event[\"start_time\"])\n",
    "        end = float(event[\"end_time\"])\n",
    "        for q in event.get(\"conversations\", []):\n",
    "            question_text = q.get(\"question\")\n",
    "            choices = {k: q[k] for k in ['a', 'b', 'c', 'd'] if k in q}\n",
    "            correct = q.get(\"correct\")\n",
    "            \n",
    "            frames = extract_frames([video_path], start, end)\n",
    "            answer = final_answer(model, frames, question_text, choices, processor, tokenizer)\n",
    "            is_correct = (answer == correct)\n",
    "            \n",
    "            results.append({\n",
    "                \"perspective\": \"vehicle_view\",\n",
    "                \"question\": question_text,\n",
    "                \"choices\": choices,\n",
    "                \"correct\": correct,\n",
    "                \"model_answer\": answer,\n",
    "                \"is_correct\": is_correct,\n",
    "            })\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4686f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_overhead(json_data, video_root, model):\n",
    "    results = []\n",
    "    overhead_videos = json_data[0].get(\"overhead_videos\", [])\n",
    "    \n",
    "    video_dirname = os.path.basename(json_path).replace(\".json\", \"\")\n",
    "    video_prefix = video_dirname.split(\"_\")[0:4]\n",
    "    video_folder = \"_\".join(video_prefix)\n",
    "    video_dir = os.path.join(video_root, \"train\", video_folder, \"overhead_view\")\n",
    "    videos_path = [os.path.join(video_dir, vid) for vid in overhead_videos]\n",
    "    \n",
    "    event_phases = json_data[0].get(\"event_phase\", [])\n",
    "    for phase in event_phases:\n",
    "        start = float(phase.get(\"start_time\", 0))\n",
    "        end = float(phase.get(\"end_time\", 0))\n",
    "        for conv in phase.get(\"conversations\", []):\n",
    "            question_text = conv.get(\"question\")\n",
    "            correct = conv.get(\"correct\")\n",
    "            choices = {k: conv[k] for k in ['a', 'b', 'c', 'd'] if k in conv}\n",
    "            \n",
    "            frames = extract_frames(videos_path, start, end)\n",
    "            answer = final_answer(model, frames, question_text, choices, processor, tokenizer)\n",
    "            is_correct = (answer == correct)\n",
    "            \n",
    "            results.append({\n",
    "                \"perspective\": \"overhead\",\n",
    "                \"question\": question_text,\n",
    "                \"choices\": choices,\n",
    "                \"correct\": correct,\n",
    "                \"model_answer\": answer,\n",
    "                \"is_correct\": is_correct,\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7c4df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_environment(json_data, video_root, model):\n",
    "    results = []\n",
    "    overhead_videos = json_data[0].get(\"overhead_videos\", [])\n",
    "    \n",
    "    video_dirname = os.path.basename(json_path).replace(\".json\", \"\")\n",
    "    video_prefix = video_dirname.split(\"_\")[0:4]\n",
    "    video_folder = \"_\".join(video_prefix)\n",
    "    video_dir = os.path.join(video_root, \"train\", video_folder, \"overhead_view\")\n",
    "    videos_path = [os.path.join(video_dir, vid) for vid in overhead_videos]\n",
    "    \n",
    "    questions = json_data[0].get(\"environment\", [])\n",
    "    for q in questions:\n",
    "        question_text = q.get(\"question\")\n",
    "        correct = q.get(\"correct\")\n",
    "        choices = {k: q[k] for k in ['a', 'b', 'c', 'd'] if k in q}\n",
    "    \n",
    "        frames = extract_frames(videos_path)\n",
    "        answer = final_answer(model, frames, question_text, choices, processor, tokenizer)\n",
    "        is_correct = (answer == correct)\n",
    "        \n",
    "        results.append({\n",
    "            \"perspective\": \"environment\",\n",
    "            \"question\": question_text,\n",
    "            \"choices\": choices,\n",
    "            \"correct\": correct,\n",
    "            \"model_answer\": answer,\n",
    "            \"is_correct\": is_correct,\n",
    "        })\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "470adcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing overhead_view\n",
      "Processing vehicle_view\n",
      "Processing environment\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"data/annotations/vqa/train/20230707_8_SN46_T1\"\n",
    "video_root = \"data/videos\"\n",
    "\n",
    "results = run_vqa_on_scenario(base_dir, video_root, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8bd6e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_json(results, output_path):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"[INFO] Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44cd9d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Results saved to: outputs/full_vqa_results_test3.json\n"
     ]
    }
   ],
   "source": [
    "output_json = \"outputs/full_vqa_results_test3.json\"\n",
    "save_results_to_json(results, output_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
