{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "288bbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"data/annotations/caption/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_caption.json\"\n",
    "video_root = \"data/videos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d20283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rornelas/Desktop/Mi3_Lab/AI_CIY_CHALLENGE/llama3-2b/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration,AutoModelForCausalLM \n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eac1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Videollama3\n",
    "device = \"cuda:0\"\n",
    "model_path = \"DAMO-NLP-SG/VideoLLaMA3-2B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    device_map={\"\": device},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7abe9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.is_available())\n",
    "# print(torch.version.cuda)\n",
    "# print(torch.cuda.device_count())\n",
    "# if torch.cuda.is_available():\n",
    "#     print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc189d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf351cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llava\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_id = \"llava-hf/llava-1.5-13b-hf\"\n",
    "# processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "# tokenizer = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "# model = LlavaForConditionalGeneration.from_pretrained(\n",
    "#     model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    "# ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5391c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_caption(frames, prompt):\n",
    "#     captions = []\n",
    "#     for frame in frames:\n",
    "#         inputs = processor(images=frame, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "#         inputs = {k: v.to(torch.bfloat16) if v.dtype == torch.float32 else v for k, v in inputs.items()}\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "#         caption = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "#         captions.append(caption)\n",
    "        \n",
    "#      return \" \".join(captions)\n",
    "\n",
    "def generate_caption_batch(frames, prompt):\n",
    "    # Process all frames together with the prompt\n",
    "    inputs = processor(images=frames, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Convert float tensors to bfloat16 if needed to match model dtype\n",
    "    inputs = {\n",
    "        k: (v.to(torch.bfloat16) if isinstance(v, torch.Tensor) and v.dtype == torch.float32 else v.to(model.device) if isinstance(v, torch.Tensor) else v)\n",
    "        for k, v in inputs.items()\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "        \n",
    "    caption = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return caption\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc6b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_middle_frame(video_path, start_time, end_time):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"[ERROR] Cannot open video: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 0:\n",
    "        cap.release()\n",
    "        return None\n",
    "    \n",
    "    middle_time = (start_time + end_time) / 2.0\n",
    "    frame_number = int(middle_time * fps)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"[ERROR] Failed to read frame at {frame_number} in video: {video_path}\")\n",
    "        return None\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(frame_rgb)\n",
    "    return pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e466630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_frames(video_paths, start_time = 0.0, end_time=None, interval=1.0):\n",
    "#     all_frames = []\n",
    "#     for video_path in video_paths:\n",
    "#         cap = cv2.VideoCapture(str(video_path))\n",
    "#         if not cap.isOpened():\n",
    "#             print(f\"Cannot open video: {video_path}\")\n",
    "#             continue\n",
    "        \n",
    "#         fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "#         if fps == 0:\n",
    "#             cap.release()\n",
    "#             continue\n",
    "        \n",
    "#         total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#         video_duration = total_frames/fps\n",
    "        \n",
    "#         if end_time is None or end_time > video_duration:\n",
    "#             end_time = video_duration\n",
    "        \n",
    "#         frame_interval = int(fps*interval)\n",
    "#         start_frame = int(start_time * fps)\n",
    "#         end_frame = int(end_time * fps)\n",
    "\n",
    "#         for frame_num in range(start_frame, end_frame, frame_interval):\n",
    "#             cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "#             success, frame = cap.read()\n",
    "#             if not success:\n",
    "#                 continue\n",
    "#             frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#             pil_image = Image.fromarray(frame_rgb)\n",
    "#             all_frames.append(pil_image)\n",
    "#         cap.release()\n",
    "#     return all_frames\n",
    "\n",
    "def extract_frames(video_paths, start_time=0.0, end_time=None, interval=1.0):\n",
    "    all_frames = []\n",
    "    for video_path in video_paths:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Cannot open video: {video_path}\")\n",
    "            continue\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if fps == 0:\n",
    "            cap.release()\n",
    "            continue\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        video_duration = total_frames / fps\n",
    "        \n",
    "        if end_time is None or end_time > video_duration:\n",
    "            end_time = video_duration\n",
    "        \n",
    "        frame_interval = int(fps * interval)\n",
    "        start_frame = int(start_time * fps)\n",
    "        end_frame = int(end_time * fps)\n",
    "\n",
    "        for frame_num in range(start_frame, end_frame, frame_interval):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                continue\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame_rgb)\n",
    "\n",
    "            # Append a dict with metadata + image\n",
    "            all_frames.append({\n",
    "                \"video_path\": video_path,\n",
    "                \"fps\": fps,\n",
    "                \"frame_num\": frame_num,\n",
    "                \"image\": pil_image\n",
    "            })\n",
    "        cap.release()\n",
    "    return all_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c2db44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_video_path(video_root, video_file):\n",
    "    for subfolder in [\"train\", \"val\"]:\n",
    "        folder_path = os.path.join(video_root, subfolder)\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            if video_file in files:\n",
    "                return os.path.join(root, video_file)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2239fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_video(video_root: str, video_file: str):\n",
    "    for dirpath, _, files in os.walk(video_root):\n",
    "        if video_file in files:\n",
    "            return os.path.join(dirpath, video_file)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6a4c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_on_json(json_path: str, video_root: str):\n",
    "#     with open(json_path, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     video_files = data.get(\"overhead_videos\", [])\n",
    "#     events = data.get(\"event_phase\", [])\n",
    "\n",
    "#     if not video_files or not events:\n",
    "#         print(f\"[SKIP] Invalid video/event alignment in {json_path}\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"[INFO] Processing: {os.path.basename(json_path)}\")\n",
    "\n",
    "#     for event in events:\n",
    "#         start = float(event[\"start_time\"])\n",
    "#         end = float(event[\"end_time\"])\n",
    "#         labels = event.get(\"labels\", [])\n",
    "\n",
    "#         all_frames = []\n",
    "#         for video_file in video_files:\n",
    "#             video_paths = [find_video(video_root, vf) for vf in video_files]\n",
    "#             frames = extract_frames(video_paths, start, end)\n",
    "#             if frames:\n",
    "#                 all_frames.extend(frames)\n",
    "\n",
    "#         if not all_frames:\n",
    "#             continue\n",
    "\n",
    "#         caption_ped = generate_caption(\n",
    "#             all_frames,\n",
    "#             \"<image>  Describe the crash victim in detail: age, gender, clothing, posture, and behavior. Were they distracted or alert? Were they in a legal crossing area? Mention their awareness of the vehicle.\"\n",
    "\n",
    "#         )\n",
    "#         caption_veh = generate_caption(\n",
    "#             all_frames,\n",
    "#             \"<image>  Describe the vehicle involved in the accident with the crash victim. Was it accelerating, braking, or turning? Describe its position relative to the pedestrian and crosswalk. Did it yield? Was it following traffic rules?\"\n",
    "#         )\n",
    "#         print(f\"\\nEvent Labels: {labels}\")\n",
    "#         print(f\"Pedestrian: {caption_ped}\")\n",
    "#         print(f\"Vehicle: {caption_veh}\")\n",
    "\n",
    "def run_on_json(json_path: str, video_root: str):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    video_files = data.get(\"overhead_videos\", [])\n",
    "    events = data.get(\"event_phase\", [])\n",
    "\n",
    "    if not video_files or not events:\n",
    "        print(f\"[SKIP] Invalid video/event alignment in {json_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Processing: {os.path.basename(json_path)}\")\n",
    "\n",
    "    video_paths = [find_video(video_root, vf) for vf in video_files]\n",
    "\n",
    "\n",
    "    for event in events:\n",
    "        start = float(event[\"start_time\"])\n",
    "        end = float(event[\"end_time\"])\n",
    "        labels = event.get(\"labels\", [])\n",
    "\n",
    "        all_frames = []\n",
    "        for video_path in video_paths:\n",
    "            frames = extract_frames([video_path], start, end)\n",
    "            if frames:\n",
    "                all_frames.extend(frames)\n",
    "\n",
    "        if not all_frames:\n",
    "            continue\n",
    "        \n",
    "        pedestrian_prompt = ( \"<image>\"\n",
    "        \" Describe the crash victim: age, gender, clothing, posture, \"\n",
    "        \"behavior, alertness, and crossing legality.\"\n",
    "        )\n",
    "        vehicle_prompt = ( \"<image>\"\n",
    "        \" Describe the vehicle involved: movement, position relative to pedestrian, \"\n",
    "        \"and compliance with traffic rules.\"\n",
    "        )\n",
    "\n",
    "        caption_ped = generate_caption_batch(all_frames, pedestrian_prompt)\n",
    "        caption_veh = generate_caption_batch(all_frames, vehicle_prompt)\n",
    "\n",
    "        print(f\"\\nEvent Labels: {labels}\")\n",
    "        print(f\"Pedestrian: {caption_ped}\")\n",
    "        print(f\"Vehicle: {caption_veh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2434afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions(json_path, video_root):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    video_files = data.get(\"overhead_videos\", [])\n",
    "    events = data.get(\"event_phase\", [])\n",
    "\n",
    "    if not video_files or not events:\n",
    "        print(f\"[SKIP] Invalid video/event alignment in {json_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Processing: {os.path.basename(json_path)}\")\n",
    "\n",
    "    video_paths = [find_video(video_root, vf) for vf in video_files]\n",
    "\n",
    "    for event in events:\n",
    "        start = float(event[\"start_time\"])\n",
    "        end = float(event[\"end_time\"])\n",
    "        labels = event.get(\"labels\", [])\n",
    "\n",
    "        all_frames = []\n",
    "        for video_path in video_paths:\n",
    "            frames = extract_frames([video_path], start, end)\n",
    "            if frames:\n",
    "                all_frames.extend(frames)\n",
    "\n",
    "        if not all_frames:\n",
    "            continue\n",
    "\n",
    "        all_responses = []\n",
    "\n",
    "        for i, segment in enumerate(all_frames):\n",
    "            video_path = segment[\"video_path\"]\n",
    "            fps = segment.get(\"fps\", 30)  # default fps\n",
    "            max_frames = segment.get(\"max_frames\", int(fps * (end - start)))  # better default max_frames\n",
    "\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "                    You are an AI assistant analyzing traffic video segments. \n",
    "                    For each video segment, generate two captions:\n",
    "                    1. Describe pedestrians' behavior, positions, and interactions.\n",
    "                    2. Describe vehicles' movements, positions, and interactions.\n",
    "                    Provide clear, concise captions focusing on relevant traffic events, including accidents or normal traffic flow.\n",
    "                    Use relative positions and timing if applicable.\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"video\", \"video\": {\"video_path\": video_path, \"fps\": fps, \"max_frames\": max_frames}},\n",
    "                        {\"type\": \"text\", \"text\": \"\"\"\n",
    "                        For this video segment, provide two captions:\n",
    "                        Pedestrian Caption: [Describe all pedestrian activity]\n",
    "                        Vehicle Caption: [Describe all vehicle activity]\n",
    "                        Include mentions of any accidents or noteworthy behavior.\n",
    "                        \"\"\"}\n",
    "                    ]\n",
    "                },\n",
    "            ]\n",
    "\n",
    "            inputs = processor(\n",
    "                conversation=conversation,\n",
    "                add_system_prompt=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "            if \"pixel_values\" in inputs:\n",
    "                inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(torch.bfloat16)\n",
    "\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "            response = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "            pedestrian_caption = \"\"\n",
    "            vehicle_caption = \"\"\n",
    "\n",
    "            # Parse the response\n",
    "            for line in response.splitlines():\n",
    "                if line.lower().startswith(\"pedestrian caption:\"):\n",
    "                    pedestrian_caption = line[len(\"pedestrian caption:\"):].strip()\n",
    "                elif line.lower().startswith(\"vehicle caption:\"):\n",
    "                    vehicle_caption = line[len(\"vehicle caption:\"):].strip()\n",
    "\n",
    "            if not pedestrian_caption or not vehicle_caption:\n",
    "                parts = response.split('\\n\\n')\n",
    "                if len(parts) >= 2:\n",
    "                    pedestrian_caption = parts[0].strip()\n",
    "                    vehicle_caption = parts[1].strip()\n",
    "\n",
    "            all_responses.append({\n",
    "                \"segment_index\": i,\n",
    "                \"video_path\": video_path,\n",
    "                \"pedestrian_caption\": pedestrian_caption,\n",
    "                \"vehicle_caption\": vehicle_caption,\n",
    "                \"raw_response\": response,\n",
    "            })\n",
    "\n",
    "        # You can choose to return all_responses here or accumulate them elsewhere\n",
    "        return all_responses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0408c6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: 20230707_8_SN46_T1_caption.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'segment_index': 0,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4',\n",
       "  'pedestrian_caption': 'A pedestrian is crossing the street in front of a car. The pedestrian and driver are both moving towards each other.',\n",
       "  'vehicle_caption': 'A black vehicle approaches from behind the traffic light, followed by a silver convertible driving on the road.',\n",
       "  'raw_response': 'Pedestrian Caption: A pedestrian is crossing the street in front of a car. The pedestrian and driver are both moving towards each other.\\nVehicle Caption: A black vehicle approaches from behind the traffic light, followed by a silver convertible driving on the road.'},\n",
       " {'segment_index': 1,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4',\n",
       "  'pedestrian_caption': 'A man in a yellow shirt and black pants walks across the crosswalk. Another person, wearing a dark outfit, is seen walking towards the traffic light pole.',\n",
       "  'vehicle_caption': 'Cars are passing by on the road, including a silver car parked on the side of the street.',\n",
       "  'raw_response': 'Pedestrian Caption: A man in a yellow shirt and black pants walks across the crosswalk. Another person, wearing a dark outfit, is seen walking towards the traffic light pole.\\nVehicle Caption: Cars are passing by on the road, including a silver car parked on the side of the street.'},\n",
       " {'segment_index': 2,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4',\n",
       "  'pedestrian_caption': 'A woman wearing a black dress is crossing the road from left to right. She is followed by two people, one of whom is carrying a child in their arms.',\n",
       "  'vehicle_caption': 'The traffic light changes and a black car turns right at the intersection while another black car drives through the roundabout ahead.',\n",
       "  'raw_response': 'Pedestrian Caption: A woman wearing a black dress is crossing the road from left to right. She is followed by two people, one of whom is carrying a child in their arms.\\nVehicle Caption: The traffic light changes and a black car turns right at the intersection while another black car drives through the roundabout ahead.'},\n",
       " {'segment_index': 3,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4',\n",
       "  'pedestrian_caption': 'A man in a dark shirt walks across the crosswalk, followed by another pedestrian who is walking towards the right side of the frame. There are no visible accidents or notable behavior.',\n",
       "  'vehicle_caption': 'Several cars pass through the intersection, including a black car and a white van. The traffic light changes from green to red, but there are no accidents or significant events observed during this time.',\n",
       "  'raw_response': 'Pedestrian Caption: A man in a dark shirt walks across the crosswalk, followed by another pedestrian who is walking towards the right side of the frame. There are no visible accidents or notable behavior.\\nVehicle Caption: Several cars pass through the intersection, including a black car and a white van. The traffic light changes from green to red, but there are no accidents or significant events observed during this time.'},\n",
       " {'segment_index': 4,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4',\n",
       "  'pedestrian_caption': 'There are two pedestrians visible in the video. One is walking towards the left side of the frame while the other is standing near the car on the right side.',\n",
       "  'vehicle_caption': 'A black car and a silver convertible are seen driving through the intersection, while another car stops at the red light.',\n",
       "  'raw_response': 'Pedestrian Caption: There are two pedestrians visible in the video. One is walking towards the left side of the frame while the other is standing near the car on the right side.\\nVehicle Caption: A black car and a silver convertible are seen driving through the intersection, while another car stops at the red light.'},\n",
       " {'segment_index': 5,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera1_0.mp4',\n",
       "  'pedestrian_caption': 'Two pedestrians are crossing the road, one in a white shirt and black pants and the other in a black shirt and blue jeans. They both walk across the crosswalk from right to left.',\n",
       "  'vehicle_caption': \"A silver car is stopped on the side of the road with its door open. In the background, there's a green van parked near some trees.\",\n",
       "  'raw_response': \"Pedestrian Caption: Two pedestrians are crossing the road, one in a white shirt and black pants and the other in a black shirt and blue jeans. They both walk across the crosswalk from right to left.\\nVehicle Caption: A silver car is stopped on the side of the road with its door open. In the background, there's a green van parked near some trees.\"},\n",
       " {'segment_index': 6,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera2_1.mp4',\n",
       "  'pedestrian_caption': 'A woman in a white shirt and black pants is crossing the street. She is walking between two yellow and black striped cones on the road.',\n",
       "  'vehicle_caption': 'There are several cars driving on the road, including a blue car that enters the frame from the left side of the screen and stops near the center of the screen. Another car drives by on the right side of the screen.',\n",
       "  'raw_response': 'Pedestrian Caption: A woman in a white shirt and black pants is crossing the street. She is walking between two yellow and black striped cones on the road.\\nVehicle Caption: There are several cars driving on the road, including a blue car that enters the frame from the left side of the screen and stops near the center of the screen. Another car drives by on the right side of the screen.'},\n",
       " {'segment_index': 7,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera2_1.mp4',\n",
       "  'pedestrian_caption': 'A woman in a white shirt and black shorts is crossing the road. She appears to be walking with caution, possibly due to the presence of traffic cones on the road.',\n",
       "  'vehicle_caption': 'Several cars are driving slowly through an intersection. There are two vehicles that appear to have had an accident as one car is stopped behind another vehicle.',\n",
       "  'raw_response': 'Pedestrian Caption: A woman in a white shirt and black shorts is crossing the road. She appears to be walking with caution, possibly due to the presence of traffic cones on the road.\\nVehicle Caption: Several cars are driving slowly through an intersection. There are two vehicles that appear to have had an accident as one car is stopped behind another vehicle.'},\n",
       " {'segment_index': 8,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera2_1.mp4',\n",
       "  'pedestrian_caption': 'A woman walks across the parking lot while a man is standing on the side of the road.',\n",
       "  'vehicle_caption': 'The blue car drives into the parking lot, and the black van stops in front of it.',\n",
       "  'raw_response': 'Pedestrian Caption: A woman walks across the parking lot while a man is standing on the side of the road.\\nVehicle Caption: The blue car drives into the parking lot, and the black van stops in front of it.'},\n",
       " {'segment_index': 9,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera2_1.mp4',\n",
       "  'pedestrian_caption': 'A man is crossing the road from left to right. Another person is standing on the side of the road, watching him.',\n",
       "  'vehicle_caption': 'A blue car approaches and stops near a minivan. The blue car then drives away.',\n",
       "  'raw_response': 'Pedestrian Caption: A man is crossing the road from left to right. Another person is standing on the side of the road, watching him. \\nVehicle Caption: A blue car approaches and stops near a minivan. The blue car then drives away.'},\n",
       " {'segment_index': 10,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera2_1.mp4',\n",
       "  'pedestrian_caption': 'A woman is crossing the street in front of a van. She appears to be running across the road, carefully navigating around two cones placed on the pavement.',\n",
       "  'vehicle_caption': 'The van remains stationary throughout the video. Other vehicles pass by, including a blue car that approaches from behind and stops briefly before moving away.',\n",
       "  'raw_response': 'Pedestrian Caption: A woman is crossing the street in front of a van. She appears to be running across the road, carefully navigating around two cones placed on the pavement.\\nVehicle Caption: The van remains stationary throughout the video. Other vehicles pass by, including a blue car that approaches from behind and stops briefly before moving away.'},\n",
       " {'segment_index': 11,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera2_1.mp4',\n",
       "  'pedestrian_caption': 'A woman in a white top and dark pants is walking across the road, while two other people are standing on the sidewalk. There is also a man wearing a blue shirt who walks towards the center of the frame.',\n",
       "  'vehicle_caption': 'Several cars are driving through the intersection, including a black car that turns left onto another street, followed by a maroon van turning right onto the same street. The traffic flow appears to be normal without any accidents or notable behavior.',\n",
       "  'raw_response': 'Pedestrian Caption: A woman in a white top and dark pants is walking across the road, while two other people are standing on the sidewalk. There is also a man wearing a blue shirt who walks towards the center of the frame.\\nVehicle Caption: Several cars are driving through the intersection, including a black car that turns left onto another street, followed by a maroon van turning right onto the same street. The traffic flow appears to be normal without any accidents or notable behavior.'},\n",
       " {'segment_index': 12,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera2_2.mp4',\n",
       "  'pedestrian_caption': 'A woman is seen crossing the road while holding a phone. She appears to be looking at her phone and then continues walking.',\n",
       "  'vehicle_caption': 'Several cars are driving in different directions on the road, including a blue car that enters from the right side of the frame and stops near the van.',\n",
       "  'raw_response': 'Pedestrian Caption: A woman is seen crossing the road while holding a phone. She appears to be looking at her phone and then continues walking.\\nVehicle Caption: Several cars are driving in different directions on the road, including a blue car that enters from the right side of the frame and stops near the van.'},\n",
       " {'segment_index': 13,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera2_2.mp4',\n",
       "  'pedestrian_caption': 'A woman is walking across the street, while a man and two children are standing on the sidewalk. They seem to be waiting for something.',\n",
       "  'vehicle_caption': 'There are several vehicles in the video, including cars, vans, and trucks. Some of them are parked or stopped at traffic lights, while others are driving slowly through the intersection. One car drives by quickly, narrowly missing the pedestrians.',\n",
       "  'raw_response': 'Pedestrian Caption: A woman is walking across the street, while a man and two children are standing on the sidewalk. They seem to be waiting for something.\\nVehicle Caption: There are several vehicles in the video, including cars, vans, and trucks. Some of them are parked or stopped at traffic lights, while others are driving slowly through the intersection. One car drives by quickly, narrowly missing the pedestrians.'},\n",
       " {'segment_index': 14,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera2_2.mp4',\n",
       "  'pedestrian_caption': 'A woman is crossing the road in front of a van. She stops to look at something on her phone, then continues walking.',\n",
       "  'vehicle_caption': 'A blue car drives past and makes a right turn. The van follows behind it.',\n",
       "  'raw_response': 'Pedestrian Caption: A woman is crossing the road in front of a van. She stops to look at something on her phone, then continues walking.\\nVehicle Caption: A blue car drives past and makes a right turn. The van follows behind it.'},\n",
       " {'segment_index': 15,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera2_2.mp4',\n",
       "  'pedestrian_caption': 'A woman is crossing the street while holding a phone. She walks from the left side of the frame to the right, passing in front of a yellow and black striped cone.',\n",
       "  'vehicle_caption': \"The blue car drives towards the van and stops behind it. The man opens the driver's door and gets into the vehicle.\",\n",
       "  'raw_response': \"Pedestrian Caption: A woman is crossing the street while holding a phone. She walks from the left side of the frame to the right, passing in front of a yellow and black striped cone.\\nVehicle Caption: The blue car drives towards the van and stops behind it. The man opens the driver's door and gets into the vehicle.\"},\n",
       " {'segment_index': 16,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera2_2.mp4',\n",
       "  'pedestrian_caption': 'A woman in a white shirt and blue jeans is seen walking across the street. She appears to be crossing from the left side of the frame towards the right, passing between two yellow and black striped cones placed on the road.',\n",
       "  'vehicle_caption': 'A dark-colored sedan drives past the van and stops behind it. The sedan then moves forward and turns slightly to the left before driving away down the road.',\n",
       "  'raw_response': 'Pedestrian Caption: A woman in a white shirt and blue jeans is seen walking across the street. She appears to be crossing from the left side of the frame towards the right, passing between two yellow and black striped cones placed on the road.\\nVehicle Caption: A dark-colored sedan drives past the van and stops behind it. The sedan then moves forward and turns slightly to the left before driving away down the road.'},\n",
       " {'segment_index': 17,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera2_2.mp4',\n",
       "  'pedestrian_caption': 'A woman is walking across the road while a car passes by.',\n",
       "  'vehicle_caption': 'The van turns to go in the opposite direction.',\n",
       "  'raw_response': 'Pedestrian Caption: A woman is walking across the road while a car passes by.\\nVehicle Caption: The van turns to go in the opposite direction.'},\n",
       " {'segment_index': 18,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera3_3.mp4',\n",
       "  'pedestrian_caption': 'A man and a woman are walking across the street. They seem to be having a conversation as they walk.',\n",
       "  'vehicle_caption': 'There is a black car driving slowly in the right lane of the parking lot, while another car drives by on the left side. The cars appear to be moving at a normal speed for their lanes.',\n",
       "  'raw_response': 'Pedestrian Caption: A man and a woman are walking across the street. They seem to be having a conversation as they walk.\\nVehicle Caption: There is a black car driving slowly in the right lane of the parking lot, while another car drives by on the left side. The cars appear to be moving at a normal speed for their lanes.'},\n",
       " {'segment_index': 19,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera3_3.mp4',\n",
       "  'pedestrian_caption': 'Two people are walking towards the center of the parking lot.',\n",
       "  'vehicle_caption': 'A black car enters from the left and parks in the right lane.',\n",
       "  'raw_response': 'Pedestrian Caption: Two people are walking towards the center of the parking lot.\\nVehicle Caption: A black car enters from the left and parks in the right lane.'},\n",
       " {'segment_index': 20,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera3_3.mp4',\n",
       "  'pedestrian_caption': 'Two pedestrians are walking in the parking lot, one is wearing a white shirt and black pants, while the other is wearing a red shirt and blue jeans. They seem to be crossing the street from left to right.',\n",
       "  'vehicle_caption': 'A dark-colored car drives into view from the left side of the frame and stops at the designated parking spot. After a brief pause, it turns right and exits the frame on the right side.',\n",
       "  'raw_response': 'Pedestrian Caption: Two pedestrians are walking in the parking lot, one is wearing a white shirt and black pants, while the other is wearing a red shirt and blue jeans. They seem to be crossing the street from left to right.\\nVehicle Caption: A dark-colored car drives into view from the left side of the frame and stops at the designated parking spot. After a brief pause, it turns right and exits the frame on the right side.'},\n",
       " {'segment_index': 21,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera3_3.mp4',\n",
       "  'pedestrian_caption': 'The caption for the pedestrian activity is not provided in this video segment.',\n",
       "  'vehicle_caption': 'The vehicles are seen moving around and parking, but no accidents or noteworthy behavior is mentioned.',\n",
       "  'raw_response': 'Pedestrian Caption: The caption for the pedestrian activity is not provided in this video segment.\\nVehicle Caption: The vehicles are seen moving around and parking, but no accidents or noteworthy behavior is mentioned.'},\n",
       " {'segment_index': 22,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera3_3.mp4',\n",
       "  'pedestrian_caption': 'Two pedestrians are seen walking across the parking lot. One of them is wearing a red shirt and the other one is in blue.',\n",
       "  'vehicle_caption': 'A black car enters from the left side, makes a right turn at the intersection, and exits to the right. Another car, a white one, enters from the right, makes a left turn at the intersection, and also exits to the right.',\n",
       "  'raw_response': 'Pedestrian Caption: Two pedestrians are seen walking across the parking lot. One of them is wearing a red shirt and the other one is in blue.\\nVehicle Caption: A black car enters from the left side, makes a right turn at the intersection, and exits to the right. Another car, a white one, enters from the right, makes a left turn at the intersection, and also exits to the right.'},\n",
       " {'segment_index': 23,\n",
       "  'video_path': 'data/videos/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1_Camera3_3.mp4',\n",
       "  'pedestrian_caption': 'A person wearing a blue shirt and white pants walks from the left side of the frame to the right. Another individual, dressed in black, stands near the middle of the parking lot.',\n",
       "  'vehicle_caption': 'The car on the far left remains stationary throughout the video. Later, another car drives into view from the left and parks behind the first car.',\n",
       "  'raw_response': 'Pedestrian Caption: A person wearing a blue shirt and white pants walks from the left side of the frame to the right. Another individual, dressed in black, stands near the middle of the parking lot.\\nVehicle Caption: The car on the far left remains stationary throughout the video. Later, another car drives into view from the left and parks behind the first car.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_captions(json_path, video_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "11161b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: 20230707_8_SN46_T1_caption.json\n",
      "\n",
      "Event Labels: ['4']\n",
      "Pedestrian: Describe the crash victim in detail: age, gender, clothing, posture, and behavior. Were they distracted or alert? Were they in a legal crossing area? Mention their awareness of the vehicle.\n",
      "\n",
      "The crash victim is a person wearing a white shirt. They are standing in the middle of the street, possibly waiting to cross the road. They are not in a legal crossing area, and their posture suggests that they are alert and aware of their surroundings. It is not clear if they were distracted or not, but their presence in the middle of the street indicates that they were not following traffic rules and could be at risk of being hit by a vehicle.\n",
      "Vehicle: Describe the vehicle involved in the accident with the crash victim. Was it accelerating, braking, or turning? Describe its position relative to the pedestrian and crosswalk. Did it yield? Was it following traffic rules?\n",
      "\n",
      "The vehicle involved in the accident was a black van. It was traveling on the road and was not following traffic rules. The van was not accelerating, braking, or turning. Instead, it was driving straight ahead. The van was positioned on the road, and it was not yielding to the pedestrian who was crossing the street. The pedestrian was crossing the street at a crosswalk, and the van was not following traffic rules, which led to the accident.\n",
      "\n",
      "Event Labels: ['3']\n",
      "Pedestrian: Describe the crash victim in detail: age, gender, clothing, posture, and behavior. Were they distracted or alert? Were they in a legal crossing area? Mention their awareness of the vehicle.\n",
      "\n",
      "The crash victim is a person wearing a white shirt. They are standing in the middle of the street, possibly waiting to cross the road. They are not in a legal crossing area, and their posture suggests that they are alert and aware of their surroundings. It is not clear if they were distracted or not, but their presence in the middle of the street indicates that they were not following traffic rules and could be at risk of being hit by a vehicle.\n",
      "Vehicle: Describe the vehicle involved in the accident with the crash victim. Was it accelerating, braking, or turning? Describe its position relative to the pedestrian and crosswalk. Did it yield? Was it following traffic rules?\n",
      "\n",
      "The vehicle involved in the accident was a black car. It was traveling on the road and was not following traffic rules. The car was not accelerating, braking, or turning. Instead, it was driving straight ahead. The car was positioned in the middle of the road, and it was not yielding to the pedestrian who was crossing the street. The pedestrian was crossing the street at a crosswalk, and the car did not stop for the pedestrian, resulting in a collision.\n",
      "\n",
      "Event Labels: ['2']\n",
      "Pedestrian: Describe the crash victim in detail: age, gender, clothing, posture, and behavior. Were they distracted or alert? Were they in a legal crossing area? Mention their awareness of the vehicle.\n",
      "\n",
      "The crash victim is a woman wearing a pink shirt. She is standing in the middle of the street, possibly waiting to cross the road. She is not in a legal crossing area, and her posture suggests that she is alert and aware of her surroundings. It is not clear if she was distracted or not, but her awareness of the vehicle indicates that she was at least somewhat aware of her environment.\n",
      "Vehicle: Describe the vehicle involved in the accident with the crash victim. Was it accelerating, braking, or turning? Describe its position relative to the pedestrian and crosswalk. Did it yield? Was it following traffic rules?\n",
      "\n",
      "The vehicle involved in the accident was a black van. It was traveling on the road and was not following traffic rules. The van was not accelerating, braking, or turning. Instead, it was driving straight ahead. The van was positioned in the middle of the road, and it was not yielding to the pedestrian who was crossing the street. The pedestrian was crossing the street at a crosswalk, and the van was not following traffic rules, which led to the accident.\n",
      "\n",
      "Event Labels: ['1']\n",
      "Pedestrian: Describe the crash victim in detail: age, gender, clothing, posture, and behavior. Were they distracted or alert? Were they in a legal crossing area? Mention their awareness of the vehicle.\n",
      "\n",
      "The crash victim is a woman wearing a pink shirt. She is standing in the middle of the street, possibly waiting to cross the road. She is not in a legal crossing area, and her posture suggests that she is alert and aware of her surroundings. It is unclear whether she was distracted or not, but her presence in the middle of the street poses a risk to her safety.\n",
      "Vehicle: Describe the vehicle involved in the accident with the crash victim. Was it accelerating, braking, or turning? Describe its position relative to the pedestrian and crosswalk. Did it yield? Was it following traffic rules?\n",
      "\n",
      "The vehicle involved in the accident was a black van. It was traveling on the road and was not following traffic rules. The van was not accelerating, braking, or turning. Instead, it was driving straight ahead. The van was positioned on the road, and it was not yielding to the pedestrian who was crossing the street. The pedestrian was crossing the street at a crosswalk, and the van was not following traffic rules, which led to the accident.\n",
      "\n",
      "Event Labels: ['0']\n",
      "Pedestrian: Describe the crash victim in detail: age, gender, clothing, posture, and behavior. Were they distracted or alert? Were they in a legal crossing area? Mention their awareness of the vehicle.\n",
      "\n",
      "The crash victim is a person wearing a white shirt. They are standing in the middle of the street, possibly distracted or not paying attention to their surroundings. They are not in a legal crossing area and are not aware of the vehicle approaching them.\n",
      "Vehicle: Describe the vehicle involved in the accident with the crash victim. Was it accelerating, braking, or turning? Describe its position relative to the pedestrian and crosswalk. Did it yield? Was it following traffic rules?\n",
      "\n",
      "The vehicle involved in the accident was a black car. It was traveling on the road and was not following traffic rules. The car was not accelerating, braking, or turning. Instead, it was driving straight ahead. The car was positioned on the road, and it was not in the crosswalk. The car was not following traffic rules, and it collided with the pedestrian who was crossing the street.\n"
     ]
    }
   ],
   "source": [
    "#Llava\n",
    "run_on_json(json_path, video_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35f04637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: 20230707_8_SN46_T1_caption.json\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Number of images does not match the number of image tokens in the text.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#VideoLlama3-2b\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mrun_on_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_root\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mrun_on_json\u001b[39m\u001b[34m(json_path, video_root)\u001b[39m\n\u001b[32m     72\u001b[39m pedestrian_prompt = ( \u001b[33m\"\u001b[39m\u001b[33m<image>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[33m\"\u001b[39m\u001b[33m Describe the crash victim: age, gender, clothing, posture, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mbehavior, alertness, and crossing legality.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m )\n\u001b[32m     76\u001b[39m vehicle_prompt = ( \u001b[33m\"\u001b[39m\u001b[33m<image>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[33m\"\u001b[39m\u001b[33m Describe the vehicle involved: movement, position relative to pedestrian, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mand compliance with traffic rules.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     79\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m caption_ped = \u001b[43mgenerate_caption_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpedestrian_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m caption_veh = generate_caption_batch(all_frames, vehicle_prompt)\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvent Labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mgenerate_caption_batch\u001b[39m\u001b[34m(frames, prompt)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_caption_batch\u001b[39m(frames, prompt):\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Process all frames together with the prompt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     inputs = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.to(model.device)\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Convert float tensors to bfloat16 if needed to match model dtype\u001b[39;00m\n\u001b[32m     18\u001b[39m     inputs = {\n\u001b[32m     19\u001b[39m         k: (v.to(torch.bfloat16) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor) \u001b[38;5;129;01mand\u001b[39;00m v.dtype == torch.float32 \u001b[38;5;28;01melse\u001b[39;00m v.to(model.device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m v)\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()\n\u001b[32m     21\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/DAMO-NLP-SG/VideoLLaMA3-2B/9f7acdfaa409a2ff841f9a079b6e2731b9e002b2/processing_videollama3.py:708\u001b[39m, in \u001b[36mVideollama3Qwen2Processor.__call__\u001b[39m\u001b[34m(self, text, conversation, images, return_labels, **kwargs)\u001b[39m\n\u001b[32m    706\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou cannot provide \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m\u001b[33m with \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_conversation(conversation, images, return_labels, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_plain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/DAMO-NLP-SG/VideoLLaMA3-2B/9f7acdfaa409a2ff841f9a079b6e2731b9e002b2/processing_videollama3.py:657\u001b[39m, in \u001b[36mVideollama3Qwen2Processor._process_plain\u001b[39m\u001b[34m(self, text, images, return_labels, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    655\u001b[39m     image_inputs = {}\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m text_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BatchFeature(data={**text_inputs, **image_inputs})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/DAMO-NLP-SG/VideoLLaMA3-2B/9f7acdfaa409a2ff841f9a079b6e2731b9e002b2/processing_videollama3.py:691\u001b[39m, in \u001b[36mVideollama3Qwen2Processor.process_text\u001b[39m\u001b[34m(self, text, image_inputs, **kwargs)\u001b[39m\n\u001b[32m    688\u001b[39m         image_idx += \u001b[32m1\u001b[39m\n\u001b[32m    689\u001b[39m     text = text.replace(\u001b[33m\"\u001b[39m\u001b[33m<placeholder>\u001b[39m\u001b[33m\"\u001b[39m, DEFAULT_IMAGE_TOKEN)\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(grid_sizes) == image_idx, \u001b[33m\"\u001b[39m\u001b[33mNumber of images does not match the number of image tokens in the text.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    693\u001b[39m text_inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(text, **kwargs)\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text_inputs\n",
      "\u001b[31mAssertionError\u001b[39m: Number of images does not match the number of image tokens in the text."
     ]
    }
   ],
   "source": [
    "#VideoLlama3-2b\n",
    "run_on_json(json_path, video_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41a191b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beginning the vqa\n",
    "json_path = \"data/annotations/vqa/train/20230707_8_SN46_T1\"\n",
    "video_root = \"data/videos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "471ea13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vqa_type(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, list) and all('environment' in item for item in data if isinstance(item, dict)):\n",
    "        return \"environment\"\n",
    "    \n",
    "    if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):\n",
    "        item = data[0]\n",
    "        if \"event_phase\" in item:\n",
    "            if \"overhead_videos\" in item:\n",
    "                return \"overhead_view\"\n",
    "            elif \"vehicle_view\" in item:\n",
    "                return \"vehicle_view\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ccf781b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected the question type:  overhead_view\n"
     ]
    }
   ],
   "source": [
    "json_path2 = 'data/annotations/vqa/train/20230707_8_SN46_T1/overhead_view/20230707_8_SN46_T1.json'\n",
    "test = find_vqa_type(json_path2)\n",
    "print(\"Detected the question type: \", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de1761ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, image_pil, question, choices, tokenizer, processor):\n",
    "    prompt = \"<image>\\n\" + question + \"\\nChoices:n\"\n",
    "    for key, val in choices.items():\n",
    "        prompt += f\"{key}: {val}\\n\"\n",
    "    prompt += \"Answer with the letter of the correct choice.\"\n",
    "    \n",
    "    inputs = processor(text=prompt, images=image_pil, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "        answer_text = tokenizer.decode(outputs[0], skip_special_tokens=True).lower().strip()\n",
    "    \n",
    "    for letter in ['a', 'b', 'c', 'd']:\n",
    "        if letter in answer_text:\n",
    "            return letter\n",
    "    return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dd6bff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_answer(model, frames, question, choices, processor, tokenizer):\n",
    "    votes = {}\n",
    "    \n",
    "    for frame in frames:\n",
    "        answer = generate_answer(model,frame, question, choices, tokenizer, processor)\n",
    "        if answer:\n",
    "            votes[answer] = votes.get(answer, 0) + 1\n",
    "            \n",
    "    if not votes:\n",
    "        return None\n",
    "\n",
    "    return max(votes.items(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d3f080e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vqa_on_scenario(base_dir, video_root, model):\n",
    "    all_results = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        if any(p in root for p in ['environment', 'overhead_view', 'vehicle_view']):\n",
    "            for file in files:\n",
    "                if file.endswith(\".json\"):\n",
    "                    perspective = os.path.basename(root)\n",
    "                    scenario_id = file.replace(\".json\", \"\")\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    \n",
    "                    print(f\"Processing {perspective}\")\n",
    "                    \n",
    "                    with open(full_path, 'r') as f:\n",
    "                        json_data = json.load(f)\n",
    "                    \n",
    "                    if perspective == \"environment\":\n",
    "                        result = evaluate_environment(json_data, video_root, model)\n",
    "                    elif perspective == \"overhead_view\":\n",
    "                        result = evaluate_overhead(json_data, video_root, model)\n",
    "                    elif perspective == \"vehicle_view\":\n",
    "                        result = evaluate_vehicle(json_data, video_root, model)\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        \"scenario\": scenario_id,\n",
    "                        \"perspective\": perspective,\n",
    "                        \"results\": result\n",
    "                        })   \n",
    "    return all_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b2d39f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vehicle(json_data, video_root, model):\n",
    "    results = []\n",
    "    video_filename = json_data[0].get(\"vehicle_view\")\n",
    "    video_path = find_video(video_root, video_filename)\n",
    "    \n",
    "    for event in json_data[0].get(\"event_phase\", []):\n",
    "        start = float(event[\"start_time\"])\n",
    "        end = float(event[\"end_time\"])\n",
    "        for q in event.get(\"conversations\", []):\n",
    "            question_text = q.get(\"question\")\n",
    "            choices = {k: q[k] for k in ['a', 'b', 'c', 'd'] if k in q}\n",
    "            correct = q.get(\"correct\")\n",
    "            \n",
    "            frames = extract_frames([video_path], start, end)\n",
    "            answer = final_answer(model, frames, question_text, choices, processor, tokenizer)\n",
    "            is_correct = (answer == correct)\n",
    "            \n",
    "            results.append({\n",
    "                \"perspective\": \"vehicle_view\",\n",
    "                \"question\": question_text,\n",
    "                \"choices\": choices,\n",
    "                \"correct\": correct,\n",
    "                \"model_answer\": answer,\n",
    "                \"is_correct\": is_correct,\n",
    "            })\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f4686f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_overhead(json_data, video_root, model):\n",
    "    results = []\n",
    "    overhead_videos = json_data[0].get(\"overhead_videos\", [])\n",
    "    \n",
    "    video_dirname = os.path.basename(json_path).replace(\".json\", \"\")\n",
    "    video_prefix = video_dirname.split(\"_\")[0:4]\n",
    "    video_folder = \"_\".join(video_prefix)\n",
    "    video_dir = os.path.join(video_root, \"train\", video_folder, \"overhead_view\")\n",
    "    videos_path = [os.path.join(video_dir, vid) for vid in overhead_videos]\n",
    "    \n",
    "    event_phases = json_data[0].get(\"event_phase\", [])\n",
    "    for phase in event_phases:\n",
    "        start = float(phase.get(\"start_time\", 0))\n",
    "        end = float(phase.get(\"end_time\", 0))\n",
    "        for conv in phase.get(\"conversations\", []):\n",
    "            question_text = conv.get(\"question\")\n",
    "            correct = conv.get(\"correct\")\n",
    "            choices = {k: conv[k] for k in ['a', 'b', 'c', 'd'] if k in conv}\n",
    "            \n",
    "            frames = extract_frames(videos_path, start, end)\n",
    "            answer = final_answer(model, frames, question_text, choices, processor, tokenizer)\n",
    "            is_correct = (answer == correct)\n",
    "            \n",
    "            results.append({\n",
    "                \"perspective\": \"overhead\",\n",
    "                \"question\": question_text,\n",
    "                \"choices\": choices,\n",
    "                \"correct\": correct,\n",
    "                \"model_answer\": answer,\n",
    "                \"is_correct\": is_correct,\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e7c4df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_environment(json_data, video_root, model):\n",
    "    results = []\n",
    "    overhead_videos = json_data[0].get(\"overhead_videos\", [])\n",
    "    \n",
    "    video_dirname = os.path.basename(json_path).replace(\".json\", \"\")\n",
    "    video_prefix = video_dirname.split(\"_\")[0:4]\n",
    "    video_folder = \"_\".join(video_prefix)\n",
    "    video_dir = os.path.join(video_root, \"train\", video_folder, \"overhead_view\")\n",
    "    videos_path = [os.path.join(video_dir, vid) for vid in overhead_videos]\n",
    "    \n",
    "    questions = json_data[0].get(\"environment\", [])\n",
    "    for q in questions:\n",
    "        question_text = q.get(\"question\")\n",
    "        correct = q.get(\"correct\")\n",
    "        choices = {k: q[k] for k in ['a', 'b', 'c', 'd'] if k in q}\n",
    "    \n",
    "        frames = extract_frames(videos_path)\n",
    "        answer = final_answer(model, frames, question_text, choices, processor, tokenizer)\n",
    "        is_correct = (answer == correct)\n",
    "        \n",
    "        results.append({\n",
    "            \"perspective\": \"environment\",\n",
    "            \"question\": question_text,\n",
    "            \"choices\": choices,\n",
    "            \"correct\": correct,\n",
    "            \"model_answer\": answer,\n",
    "            \"is_correct\": is_correct,\n",
    "        })\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "470adcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing overhead_view\n",
      "Processing vehicle_view\n",
      "Processing environment\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"data/annotations/vqa/train/20230707_8_SN46_T1\"\n",
    "video_root = \"data/videos\"\n",
    "\n",
    "results = run_vqa_on_scenario(base_dir, video_root, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8bd6e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_json(results, output_path):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"[INFO] Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "44cd9d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Results saved to: outputs/full_vqa_results_test.json\n"
     ]
    }
   ],
   "source": [
    "output_json = \"outputs/full_vqa_results_test.json\"\n",
    "save_results_to_json(results, output_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3-2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
